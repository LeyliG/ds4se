{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nbdev.showdoc import *\n",
    "from ds4se.mgmnt.prep.bpe import *\n",
    "from ds4se.exp.info import *\n",
    "from ds4se.desc.stats import *\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TraceLinkValue(source, target, technique):\n",
    "    if (technique == \"VSM\"):\n",
    "        pass\n",
    "    if (technique == \"LDA\"):\n",
    "        pass\n",
    "    if (technique == \"orthogonal\"):\n",
    "        pass\n",
    "    if (technique == \"LSA\"):\n",
    "        pass\n",
    "    if (technique == \"JS\"):\n",
    "        pass\n",
    "    if (technique == \"word2vec\"):\n",
    "        pass\n",
    "    if (technique == \"doc2vec\"):\n",
    "        pass\n",
    "    value = random.randint(0,1)/100\n",
    "    \n",
    "    return value\n",
    "\n",
    "#export\n",
    "def NumDoc(source, target):\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    difference = source_doc - target_doc\n",
    "    return [source_doc, target_doc, difference, -difference]\n",
    "   \n",
    "#export\n",
    "def VocabSize(source, target):\n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    source_size = len(source_list[0])\n",
    "    target_size = len(target_list[0])\n",
    "    difference = source_size - target_size\n",
    "    return [source_size, target_size, difference, -difference]  \n",
    "\n",
    "#export\n",
    "def AverageToken(source, target):\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    \n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    \n",
    "    source_total_token = sum(source_list[0].values())\n",
    "    target_total_token = sum(target_list[0].values())\n",
    "\n",
    "    source_token = source_total_token/source_doc\n",
    "    target_token = target_total_token/target_doc\n",
    "    difference = source_token - target_token\n",
    "    return [source_token, target_token, difference, -difference]\n",
    "\n",
    "#export\n",
    "def Vocab(artifacts_df):\n",
    "    #we can add a parameter for user to specify the number of most frequent token to return\n",
    "    cnts = preprocess(artifacts_df)\n",
    "    vocab_list = cnts[0].most_common(3)\n",
    "    total = sum(cnts[0].values())\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict[vocab_list[0][0]] = [vocab_list[0][1], vocab_list[0][1]/total]\n",
    "    vocab_dict[vocab_list[1][0]] = [vocab_list[1][1], vocab_list[1][1]/total]\n",
    "    vocab_dict[vocab_list[2][0]] = [vocab_list[2][1], vocab_list[2][1]/total]\n",
    "\n",
    "    return vocab_dict\n",
    "    \n",
    "    \n",
    "#export\n",
    "def VocabShared(source, target):\n",
    "    df = pd.concat([source, target])\n",
    "    return Vocab(df) \n",
    "    \n",
    "#export\n",
    "def SharedVocabSize(source, target):\n",
    "    df = pd.concat([source, target])\n",
    "    df_counts = preprocess(df)\n",
    "    shared_size = len(df_counts[0])\n",
    "    return shared_size\n",
    "\n",
    "#export\n",
    "def MutualInformation(source, target):\n",
    "    mutual_information = random.randint(100,200)\n",
    "    return mutual_information\n",
    "    \n",
    "#export\n",
    "def CrossEntropy(source, target):\n",
    "    cross_entropy = random.randint(100,200)\n",
    "    cross_entropy = get_system_entropy_from_df(source, \"col1\",)\n",
    "    return cross_entropy\n",
    "    \n",
    "#export\n",
    "def KLDivergence(source, target):\n",
    "    divergence = random.randint(100,200)\n",
    "    return divergence\n",
    "\n",
    "#export\n",
    "def get_docs(df, spm):\n",
    "    docs = []\n",
    "    for fn in df[\"col1\"]:\n",
    "        docs += spm.EncodeAsPieces(fn)\n",
    "    return docs\n",
    "\n",
    "#export\n",
    "def get_counters(docs):\n",
    "    doc_cnts = []\n",
    "    cnt = Counter()\n",
    "    for tok in docs:\n",
    "        cnt[tok] += 1 \n",
    "        doc_cnts.append(cnt)\n",
    "    return doc_cnts\n",
    "\n",
    "#export\n",
    "def preprocess(artifacts_df):\n",
    "    spm = sp.SentencePieceProcessor()\n",
    "    output = Path('test_data\\models')\n",
    "    system_name = \"test\"\n",
    "    spm.Load(f\"{output / system_name}.model\")\n",
    "    docs = get_docs(artifacts_df,spm)\n",
    "    cnts = get_counters(docs)\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"col1\": [\"one, two two am one,\"]})\n",
    "# df2 = pd.DataFrame({\n",
    "#     \"col1\": [\"this is a requirement yes or no two two two two one,\"]})\n",
    "# frames = [df, df2]\n",
    "# df3 = pd.concat(frames)\n",
    "# VocabShared(df,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sentencepiece as sp\n",
    "# from pathlib import Path\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     \"col1\": [\"this is a sentence\", \"this is another sentence\"],\n",
    "#     \"col2\": [\"one more\", \"this is the last sentence\"],\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_docs(df, spm):\n",
    "#     docs = []\n",
    "#     for fn in df[\"col1\"]:\n",
    "#         docs += spm.EncodeAsPieces(fn)\n",
    "#     return docs\n",
    "# d = {'col1': [\"one requirement, i want this and that and this\", \"this is is is is a new requirement\"], 'col2': [3, 4]}\n",
    "# df = pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_counters(docs):\n",
    "    doc_cnts = []\n",
    "    cnt = Counter()\n",
    "    for tok in docs:\n",
    "        cnt[tok] += 1 \n",
    "        doc_cnts.append(cnt)\n",
    "    return doc_cnts\n",
    "\n",
    "req_cnts = get_counters(docs)\n",
    "list = req_cnts[0].most_common(2)\n",
    "sum(req_cnts[0].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbdev_build_docs\n",
    "# notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test case for proto TLV\n",
    "\n",
    "#Prototype should print a value between 0 or 1. Input values aren't used\n",
    "testTLV = TraceLinkValue(0,1,1)\n",
    "assert((testTLV == 0) or (testTLV == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Proto NumDoc\n",
    "\n",
    "#Prototype should print an array based on the randomly generated numbers. Input values aren't used.\n",
    "testND = NumDoc(100,10000000)\n",
    "assert(isinstance(testND, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test case for Vocab size\n",
    "\n",
    "#Prototype should return an array based on random number values. Inputs aren't used.\n",
    "testVS = VocabSize(100,100000)\n",
    "assert(isinstance(testVS, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for AverageToken\n",
    "\n",
    "#Prototype should return an array/list. Inputs aren't used\n",
    "testAT = AverageToken(100,100)\n",
    "assert(isinstance(testAT, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Vocab\n",
    "\n",
    "#Prototype Vocab should return dictionary type. \n",
    "#Will expand to test dictionary values once presets can be generated and tested\n",
    "testVocab = Vocab(\"file\")\n",
    "\n",
    "assert(isinstance(testVocab,dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test VocabShared\n",
    "\n",
    "#Prototype should show a dictionary\n",
    "#Test will be expanded to verify frequency verification in two samples\n",
    "testVocabS = VocabShared(\"source\",\"target\")\n",
    "assert(isinstance(testVocabS,dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Shared vocab size\n",
    "\n",
    "#Prototype should return an i\n",
    "testSVS = SharedVocabSize(\"source\",\"target\")\n",
    "assert(testSVS in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#TEst Mutual information\n",
    "\n",
    "#Proto mutual information should get an int\n",
    "testMutualInfo = MutualInformation(\"source\",\"target\")\n",
    "assert(testMutualInfo in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Cross Entropy\n",
    "\n",
    "#Proto cross Entropy should return an int\n",
    "testCrossEntropy = CrossEntropy(\"Sauce\",\"Target\")\n",
    "assert(testCrossEntropy in range(100,200))\n",
    "\n",
    "#Eventually cross entropy will properly implement Entropy calculation and the test will compare a known/predicted entropy\n",
    "# to a calculated value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test KLDivergence\n",
    "\n",
    "#Proto KLDivegence will return an random int\n",
    "testKLDiver = KLDivergence(\"source\",\"target\")\n",
    "assert(testKLDiver in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
