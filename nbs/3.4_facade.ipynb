{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp facade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from ds4se.mining.unsupervised.traceability.eval import *\n",
    "#import ds4se.mining.unsupervised.traceability.approach.cisco as cisco\n",
    "from enum import Enum, unique, auto\n",
    "from ds4se.exp import i\n",
    "import os\n",
    "import pkg_resources\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@unique\n",
    "class LinkType(Enum):\n",
    "    req2tc = auto()\n",
    "    req2src = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for data analysis part of library\n",
    "\n",
    "#export\n",
    "def get_docs(df, spm):\n",
    "    #param dataframe of content that need to be processed\n",
    "    #param sentence piece processor that will process the dataframe\n",
    "    #return the document list\n",
    "    docs = []\n",
    "    for fn in df[\"contents\"]:\n",
    "        docs += spm.EncodeAsPieces(fn)\n",
    "    return docs\n",
    "\n",
    "#export\n",
    "def get_counters(docs):\n",
    "    #param doc list of contents that need info on tokens\n",
    "    #return the counters object of tokens   \n",
    "    doc_cnts = []\n",
    "    cnt = Counter()\n",
    "    for tok in docs:\n",
    "        cnt[tok] += 1 \n",
    "        doc_cnts.append(cnt)\n",
    "    return doc_cnts\n",
    "\n",
    "#export\n",
    "#load sentence piece model and call two helper function to calculate token freqnency \n",
    "def preprocess(artifacts_df):\n",
    "    spm = sp.SentencePieceProcessor()\n",
    "    bpe_model_path = pkg_resources.resource_filename('ds4se', 'model/test.model')\n",
    "    spm.Load(bpe_model_path)\n",
    "    docs = get_docs(artifacts_df,spm)\n",
    "    cnts = get_counters(docs)\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TraceLinkValue(source, target, technique, word2vec_metric = \"WMD\"):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #param technique what tecchnique to use to calculate trace values\n",
    "    #return a tuple: (distance, similarity)\n",
    "    \n",
    "    #TODO make the string saved and then return the path to that string\n",
    "    sourcePath = \"path/to/something\"#this is where we save the passed in source\n",
    "    targetPath = \"./something\"\n",
    "    \n",
    "    dummy_path = pkg_resources.resource_filename('ds4se', 'model/val.csv')\n",
    "\n",
    "    \n",
    "    value = random.randint(0,1)/100\n",
    "\n",
    "    \n",
    "    if (technique == \"VSM\"):\n",
    "        pass\n",
    "    if (technique == \"LDA\"):\n",
    "        pass\n",
    "    if (technique == \"orthogonal\"):\n",
    "        pass\n",
    "    if (technique == \"LSA\"):\n",
    "        pass\n",
    "    if (technique == \"JS\"):\n",
    "        pass\n",
    "    if (technique == \"word2vec\"):\n",
    "        model_path = pkg_resources.resource_filename('ds4se', 'model/word2vec_libest.model')\n",
    "        parameter = {\n",
    "            \"vectorizationType\": VectorizationType.word2vec,\n",
    "            \"linkType\": LinkType.req2tc,\n",
    "            \"system\": 'libest',\n",
    "            \"path_to_trained_model\": model_path,\n",
    "            \"source_path\": dummy_path,\n",
    "            \"target_path\": dummy_path,\n",
    "            \"system_path\": dummy_path,\n",
    "            \"saving_path\": 'test_data/',\n",
    "            \"names\": ['Source','Target','Linked?']\n",
    "        }\n",
    "        \n",
    "        source_df = pd.DataFrame({ \"ids\": [\"source\"],  \"text\":[source]})\n",
    "        target_df = pd.DataFrame({ \"ids\": [\"target\"],  \"text\":[target]})\n",
    "        word2vec = Word2VecSeqVect(parameter)\n",
    "        word2vec.df_source = source_df\n",
    "        word2vec.df_target = target_df\n",
    "        links = [(source_df[\"ids\"][0],target_df[\"ids\"][0])]\n",
    "        if (word2vec_metric == \"SCM\"):\n",
    "            computeDistanceMetric = word2vec.computeDistanceMetric(links, metric_list = [DistanceMetric.SCM])\n",
    "        else:\n",
    "            computeDistanceMetric = word2vec.computeDistanceMetric(links, metric_list = [DistanceMetric.WMD])\n",
    "        value = (computeDistanceMetric[0][0][2],computeDistanceMetric[0][0][3])    \n",
    "        \n",
    "    if (technique == \"doc2vec\"):\n",
    "        model_path = pkg_resources.resource_filename('ds4se', 'model/doc2vec_libest.model')\n",
    "        parameter = {\n",
    "            \"vectorizationType\": VectorizationType.doc2vec,\n",
    "            \"linkType\": LinkType.req2tc,\n",
    "            \"system\": 'libest',\n",
    "            \"path_to_trained_model\": model_path,\n",
    "            \"source_path\": dummy_path,\n",
    "            \"target_path\": dummy_path,\n",
    "            \"system_path\": dummy_path,\n",
    "            \"saving_path\": 'test_data/',\n",
    "            \"names\": ['Source','Target','Linked?']\n",
    "        }\n",
    "        \n",
    "        source_df = pd.DataFrame({ \"ids\": [\"source\"],  \"text\":[source]})\n",
    "        target_df = pd.DataFrame({ \"ids\": [\"target\"],  \"text\":[target]})\n",
    "        doc2vec = Doc2VecSeqVect(params = parameter)\n",
    "        doc2vec.df_source = source_df\n",
    "        doc2vec.df_target = target_df\n",
    "        links = [(source_df[\"ids\"][0],target_df[\"ids\"][0])]\n",
    "        doc2vec.InferDoc2Vec(steps=200)\n",
    "        table = doc2vec.computeDistanceMetric( links, metric_list = [DistanceMetric.EUC] )\n",
    "        value = (table[0][0][2], table[0][0][3])\n",
    "        #The bottom is here for reference -- may not need it\n",
    "#         doc2vec.SaveLinks()\n",
    "#         #will most likely need to change this part need to change this part to a different path\n",
    "#         path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "#         doc2vec.MatchWithGroundTruth(path_to_ground_truth)\n",
    "#         doc2vec.SaveLinks(grtruth = True)\n",
    "#         #TODO find logic to LoadLink properly and display what is needed\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 09:21:36,582 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-09 09:21:36,592 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-09 09:21:36,593 : INFO : loading Doc2Vec object from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model\n",
      "2020-11-09 09:21:36,613 : INFO : loading vocabulary recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-09 09:21:36,613 : INFO : loading trainables recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-09 09:21:36,614 : INFO : loading wv recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-09 09:21:36,615 : INFO : loading docvecs recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-09 09:21:36,615 : INFO : loaded /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model\n",
      "2020-11-09 09:21:36,621 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-09 09:21:36,711 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-09 09:21:36,714 : INFO : Computed distances or similarities ('source', 'target')[[31.28299903869629, 0.030976056431477807]]\n",
      "2020-11-09 09:21:36,741 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-09 09:21:36,753 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-09 09:21:36,754 : INFO : loading Word2Vec object from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model\n",
      "2020-11-09 09:21:36,885 : INFO : loading wv recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-09 09:21:36,886 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-09 09:21:36,887 : INFO : loading vocabulary recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-09 09:21:36,887 : INFO : loading trainables recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-09 09:21:36,888 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-09 09:21:36,888 : INFO : loaded /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model\n",
      "2020-11-09 09:21:36,897 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-09 09:21:36,899 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x14162d250>\n",
      "2020-11-09 09:21:36,900 : INFO : iterating over columns in dictionary order\n",
      "2020-11-09 09:21:36,902 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-09 09:21:37,036 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-09 09:21:37,099 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-09 09:21:37,108 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-09 09:21:37,110 : INFO : built Dictionary(335 unique tokens: ['\"\",', '\");', '\"./', '(\"%', '(\"\\\\']...) from 2 documents (total 2763 corpus positions)\n",
      "2020-11-09 09:21:37,708 : INFO : Computed distances or similarities ('source', 'target')[[0.4803363918627718, 0.6755221350342244]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4803363918627718, 0.6755221350342244)\n"
     ]
    }
   ],
   "source": [
    "str1 = open('test_data/RQ11.txt', 'r').read()\n",
    "str2 = open('test_data/us4020.c', 'r').read()\n",
    "TraceLinkValue(str1,str2,\"doc2vec\")\n",
    "\n",
    "\n",
    "source_file = pd.read_csv(\"test_data/[libest-pre-req].csv\",names=['ids', 'text'], header=None, sep=' ')\n",
    "target_file = pd.read_csv(\"test_data/[libest-pre-tc].csv\",names=['ids', 'text'], header=None, sep=' ')\n",
    "source = source_file['text'][4]  \n",
    "target = target_file['text'][4]\n",
    "result = TraceLinkValue(target, source, \"word2vec\")    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = (0.5401013460499512, 0.6493079189657213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 09:22:21,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-09 09:22:21,943 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-09 09:22:21,944 : INFO : loading Doc2Vec object from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model\n",
      "2020-11-09 09:22:21,963 : INFO : loading vocabulary recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-09 09:22:21,963 : INFO : loading trainables recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-09 09:22:21,964 : INFO : loading wv recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-09 09:22:21,964 : INFO : loading docvecs recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-09 09:22:21,965 : INFO : loaded /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/doc2vec_libest.model\n",
      "2020-11-09 09:22:21,971 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-09 09:22:22,058 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-09 09:22:22,061 : INFO : Computed distances or similarities ('source', 'target')[[31.28299903869629, 0.030976056431477807]]\n",
      "2020-11-09 09:22:22,081 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-09 09:22:22,091 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-09 09:22:22,091 : INFO : loading Word2Vec object from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model\n",
      "2020-11-09 09:22:22,143 : INFO : loading wv recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-09 09:22:22,143 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-09 09:22:22,144 : INFO : loading vocabulary recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-09 09:22:22,145 : INFO : loading trainables recursively from /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-09 09:22:22,145 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-09 09:22:22,146 : INFO : loaded /Users/yangchen/Desktop/CSCI435/ds4se/ds4se/model/word2vec_libest.model\n",
      "2020-11-09 09:22:22,155 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-09 09:22:22,157 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x141ab30d0>\n",
      "2020-11-09 09:22:22,157 : INFO : iterating over columns in dictionary order\n",
      "2020-11-09 09:22:22,159 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-09 09:22:22,298 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-09 09:22:22,364 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-09 09:22:22,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-09 09:22:22,374 : INFO : built Dictionary(335 unique tokens: ['\"\",', '\");', '\"./', '(\"%', '(\"\\\\']...) from 2 documents (total 2763 corpus positions)\n",
      "2020-11-09 09:22:23,002 : INFO : Computed distances or similarities ('source', 'target')[[0.4803363918627718, 0.6755221350342244]]\n"
     ]
    }
   ],
   "source": [
    "#FacadeTest\n",
    "#Test case for proto TLV\n",
    "\n",
    "#Prototype should print a value between 0 or 1. Input values aren't used\n",
    "\n",
    "#test doc2vec\n",
    "str1 = open('test_data/RQ11.txt', 'r').read()\n",
    "str2 = open('test_data/us4020.c', 'r').read()\n",
    "testTLV = TraceLinkValue(str1,str2,\"doc2vec\")\n",
    "assert(isinstance(testTLV,tuple))\n",
    "assert(testTLV == (31.28299903869629, 0.030976056431477807))\n",
    "\n",
    "#test word2vec\n",
    "source_file = pd.read_csv(\"test_data/[libest-pre-req].csv\",names=['ids', 'text'], header=None, sep=' ')\n",
    "target_file = pd.read_csv(\"test_data/[libest-pre-tc].csv\",names=['ids', 'text'], header=None, sep=' ')\n",
    "source = source_file['text'][4]  \n",
    "target = target_file['text'][4]\n",
    "result = TraceLinkValue(target, source, \"word2vec\")    \n",
    "assert(result == (0.4803363918627718, 0.6755221350342244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def NumDoc(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return a list containing the the difference between the two files\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    difference = source_doc - target_doc\n",
    "    return [source_doc, target_doc, difference, -difference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 0, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Proto NumDoc\n",
    "\n",
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "testND = NumDoc(df, df2)\n",
    "expected = [3,3,0,0]\n",
    "assert(isinstance(testND, list))\n",
    "assert(testND[0] == expected[0])\n",
    "assert(testND[1] == expected[1])\n",
    "assert(testND[2] == expected[2])\n",
    "assert(testND[3] == expected[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VocabSize(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return a list containing the the difference between the two files in terms of vocab\n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    source_size = len(source_list[0])\n",
    "    target_size = len(target_list[0])\n",
    "    difference = source_size - target_size\n",
    "    return [source_size, target_size, difference, -difference]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 18, -1, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "VocabSize(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test case for Vocab size\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "VocabSize(df, df2)\n",
    "#Prototype should return an array based on random number values. Inputs aren't used.\n",
    "testVS = VocabSize(df1,df2)\n",
    "expected = [17, 18, -1, 1]\n",
    "assert(testVS[0] == expected[0])\n",
    "assert(testVS[1] == expected[1])\n",
    "assert(testVS[2] == expected[2])\n",
    "assert(testVS[3] == expected[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def AverageToken(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return a list containing the the difference between the two files in terms of tokens\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    \n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    \n",
    "    source_total_token = sum(source_list[0].values())\n",
    "    target_total_token = sum(target_list[0].values())\n",
    "\n",
    "    source_token = source_total_token/source_doc\n",
    "    target_token = target_total_token/target_doc\n",
    "    difference = source_token - target_token\n",
    "    return [source_token, target_token, difference, -difference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.333333333333334, 16.0, -0.6666666666666661, 0.6666666666666661]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "AverageToken(df,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for AverageToken\n",
    "\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "test = AverageToken(df,df2)\n",
    "assert(test == [15.333333333333334, 16.0, -0.6666666666666661, 0.6666666666666661])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Vocab(artifacts_df):\n",
    "    #we can add a parameter for user to specify the number of most frequent token to return\n",
    "    #param a dataframe of contents that need to be processed\n",
    "    #return a list containing the the difference between the two files in terms of vocab\n",
    "    cnts = preprocess(artifacts_df)\n",
    "    vocab_list = cnts[0].most_common(3)\n",
    "    total = sum(cnts[0].values())\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict[vocab_list[0][0]] = [vocab_list[0][1], vocab_list[0][1]/total]\n",
    "    vocab_dict[vocab_list[1][0]] = [vocab_list[1][1], vocab_list[1][1]/total]\n",
    "    vocab_dict[vocab_list[2][0]] = [vocab_list[2][1], vocab_list[2][1]/total]\n",
    "\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Vocab\n",
    "\n",
    "#Prototype Vocab should return dictionary type. \n",
    "#Will expand to test dictionary values once presets can be generated and tested\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "testVocab = Vocab(df)\n",
    "assert(isinstance(testVocab,dict))\n",
    "\n",
    "assert(testVocab == {'▁': [23, 0.5],\n",
    " 'this': [2, 0.043478260869565216],\n",
    " 'is': [2, 0.043478260869565216]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #export\n",
    "def VocabShared(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return the similarities of vocab in the files \n",
    "    df = pd.concat([source, target])\n",
    "    return Vocab(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': [52, 0.5],\n",
       " 'this': [5, 0.04807692307692308],\n",
       " 'banana': [5, 0.04807692307692308]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana banana banana banana banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "VocabShared(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test VocabShared\n",
    "\n",
    "#Prototype should show a dictionary\n",
    "#Test will be expanded to verify frequency verification in two samples\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana banana banana banana banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "testVocabS = VocabShared(df, df2)\n",
    "assert(isinstance(testVocabS,dict))\n",
    "assert(testVocabS == {'▁': [52, 0.5],\n",
    " 'this': [5, 0.04807692307692308],\n",
    " 'banana': [5, 0.04807692307692308]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SharedVocabSize(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return the similarities of vocab sizes in the files \n",
    "    df = pd.concat([source, target])\n",
    "    df_counts = preprocess(df)\n",
    "    shared_size = len(df_counts[0])\n",
    "    return shared_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "SharedVocabSize(df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Shared vocab size\n",
    "\n",
    "#Prototype should return an int\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "testSVS = SharedVocabSize(df, df)\n",
    "assert(testSVS)\n",
    "assert(testSVS == 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Dummy Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MutualInformation(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the mutual information \n",
    "    mutual_information = random.randint(100,200)\n",
    "    return mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Mutual information\n",
    "\n",
    "#Proto mutual information should get an int\n",
    "testMutualInfo = MutualInformation(\"source\",\"target\")\n",
    "assert(testMutualInfo in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a content of the source file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this this is the content of another source fil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contents\n",
       "0               this is a content of the source file\n",
       "1                                        hello world\n",
       "2  this this is the content of another source fil..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df2 = pd.DataFrame(data=target)\n",
    "i.dit_shannon(preprocess(df)[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CrossEntropy(source, target):\n",
    "    #param source a dataframe of the entire source artifact  \n",
    "    #param target a dataframe of the entire target artifact \n",
    "    #return the entropy\n",
    "    combined = source.append(target)\n",
    "    entropy = i.dit_shannon(preprocess(combined)[0])\n",
    "\n",
    "    return entropy\n",
    "    \n",
    "\n",
    "#     cross_entropy = random.randint(100,200)#looks like it is the msi funciton in the classes for word2vec or doc2vec\n",
    "#     cross_entropy = get_system_entropy_from_df(source, \"col1\",)\n",
    "#     return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-84b80b2051da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Proto cross Entropy should return an int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtestCrossEntropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sauce\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestCrossEntropy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-63854ade9623>\u001b[0m in \u001b[0;36mCrossEntropy\u001b[0;34m(source, target)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#param target a dataframe of the entire target artifact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#return the entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdit_shannon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#FacadeTest\n",
    "#Test Cross Entropy\n",
    "\n",
    "#Proto cross Entropy should return an int\n",
    "testCrossEntropy = CrossEntropy(\"Sauce\",\"Target\")\n",
    "assert(testCrossEntropy in range(100,200))\n",
    "\n",
    "#Eventually cross entropy will properly implement Entropy calculation and the test will compare a known/predicted entropy\n",
    "# to a calculated value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeArray(text):\n",
    "    print(\"text:\",text)\n",
    "    return np.fromstring(text[1:-1],sep=' ')\n",
    "    \n",
    "#export\n",
    "def KLDivergence(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the divergence  \n",
    "    \n",
    "    #we are going to use the TSNE function since this preforms the divergence\n",
    "    source_df = pd.DataFrame({ \"ids\": [\"source\"],  \"text\":[source]})\n",
    "    target_df = pd.DataFrame({ \"ids\": [\"target\"],  \"text\":[target]})\n",
    "    source_df = source_df[\"text\"].apply(makeArray)\n",
    "    target_ef = target_df[\"text\"].apply(makeArray)\n",
    "    source_df.head()\n",
    "    target_df.head()\n",
    "    divergence = random.randint(100,200)\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: source\n",
      "text: target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#FacadeTest\n",
    "#Test KLDivergence\n",
    "\n",
    "#Proto KLDivegence will return an random int\n",
    "testKLDiver = KLDivergence(\"source\",\"target\")\n",
    "assert(testKLDiver in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
