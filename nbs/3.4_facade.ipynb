{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp facade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from ds4se.mining.unsupervised.traceability.eval import *\n",
    "from enum import Enum, unique, auto\n",
    "from ds4se.exp import i\n",
    "import os\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@unique\n",
    "class LinkType(Enum):\n",
    "    req2tc = auto()\n",
    "    req2src = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for data analysis part of library\n",
    "\n",
    "#export\n",
    "def get_docs(df, spm):\n",
    "    #param dataframe of content that need to be processed\n",
    "    #param sentence piece processor that will process the dataframe\n",
    "    #return the document list\n",
    "    docs = []\n",
    "    for fn in df[\"contents\"]:\n",
    "        docs += spm.EncodeAsPieces(fn)\n",
    "    return docs\n",
    "\n",
    "#export\n",
    "def get_counters(docs):\n",
    "    #param doc list of contents that need info on tokens\n",
    "    #return the counters object of tokens   \n",
    "    doc_cnts = []\n",
    "    cnt = Counter()\n",
    "    for tok in docs:\n",
    "        cnt[tok] += 1 \n",
    "        doc_cnts.append(cnt)\n",
    "    return doc_cnts\n",
    "\n",
    "#export\n",
    "#load sentence piece model and call two helper function to calculate token freqnency \n",
    "def preprocess(artifacts_df):\n",
    "    spm = sp.SentencePieceProcessor()\n",
    "    bpe_model_path = pkg_resources.resource_filename('ds4se', 'model/test.model')\n",
    "    spm.Load(bpe_model_path)\n",
    "    docs = get_docs(artifacts_df,spm)\n",
    "    cnts = get_counters(docs)\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TraceLinkValue(source, target, technique, word2vec_metric = \"WMD\"):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #param technique what tecchnique to use to calculate trace values\n",
    "    #return a tuple: (distance, similarity)\n",
    "    \n",
    "    #TODO make the string saved and then return the path to that string\n",
    "    sourcePath = \"path/to/something\"#this is where we save the passed in source\n",
    "    targetPath = \"./something\"\n",
    "    \n",
    "    dummy_path = pkg_resources.resource_filename('ds4se', 'model/val.csv')\n",
    "\n",
    "    \n",
    "    value = random.randint(0,1)/100\n",
    "\n",
    "    \n",
    "    if (technique == \"VSM\"):\n",
    "        pass\n",
    "    if (technique == \"LDA\"):\n",
    "        pass\n",
    "    if (technique == \"orthogonal\"):\n",
    "        pass\n",
    "    if (technique == \"LSA\"):\n",
    "        pass\n",
    "    if (technique == \"JS\"):\n",
    "        pass\n",
    "    if (technique == \"word2vec\"):\n",
    "        model_path = pkg_resources.resource_filename('ds4se', 'model/word2vec_libest.model')\n",
    "        parameter = {\n",
    "            \"vectorizationType\": VectorizationType.word2vec,\n",
    "            \"linkType\": LinkType.req2tc,\n",
    "            \"system\": 'libest',\n",
    "            \"path_to_trained_model\": model_path,\n",
    "            \"source_path\": dummy_path,\n",
    "            \"target_path\": dummy_path,\n",
    "            \"system_path\": dummy_path,\n",
    "            \"saving_path\": 'test_data/',\n",
    "            \"names\": ['Source','Target','Linked?']\n",
    "        }\n",
    "        \n",
    "        source_df = pd.DataFrame({ \"ids\": [\"source\"],  \"text\":[source]})\n",
    "        target_df = pd.DataFrame({ \"ids\": [\"target\"],  \"text\":[target]})\n",
    "        word2vec = Word2VecSeqVect(parameter)\n",
    "        word2vec.df_source = source_df\n",
    "        word2vec.df_target = target_df\n",
    "        links = [(source_df[\"ids\"][0],target_df[\"ids\"][0])]\n",
    "        if (word2vec_metric == \"SCM\"):\n",
    "            computeDistanceMetric = word2vec.computeDistanceMetric(links, metric_list = [DistanceMetric.SCM])\n",
    "        else:\n",
    "            computeDistanceMetric = word2vec.computeDistanceMetric(links, metric_list = [DistanceMetric.WMD])\n",
    "        value = (computeDistanceMetric[0][0][2],computeDistanceMetric[0][0][3])    \n",
    "        \n",
    "    if (technique == \"doc2vec\"):\n",
    "        model_path = pkg_resources.resource_filename('ds4se', 'model/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model')\n",
    "        parameter = {\n",
    "            \"vectorizationType\": VectorizationType.doc2vec,\n",
    "            \"linkType\": LinkType.req2tc,\n",
    "            \"system\": 'libest',\n",
    "            \"path_to_trained_model\": model_path,\n",
    "            \"source_path\": 'test_data/val.csv',\n",
    "            \"target_path\": 'test_data/val.csv',\n",
    "            \"system_path\": 'test_data/val.csv',\n",
    "            \"saving_path\": 'test_data/',\n",
    "            \"names\": ['Source','Target','Linked?']\n",
    "        }\n",
    "        \n",
    "        source_df = pd.DataFrame({ \"ids\": [\"source\"],  \"text\":[source]})\n",
    "        target_df = pd.DataFrame({ \"ids\": [\"target\"],  \"text\":[target]})\n",
    "        doc2vec = Doc2VecSeqVect(params = parameter)\n",
    "        doc2vec.df_source = source_df\n",
    "        doc2vec.df_target = target_df\n",
    "        doc2vec.InferDoc2Vec(steps=200)\n",
    "        table = doc2vec.ComputeDistanceArtifacts( sampling=True, samples = 50, metric_list = [DistanceMetric.EUC] )\n",
    "        value = (table[0][0][2], table[0][0][3])\n",
    "        #The bottom is here for reference -- may not need it\n",
    "#         doc2vec.SaveLinks()\n",
    "#         #will most likely need to change this part need to change this part to a different path\n",
    "#         path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "#         doc2vec.MatchWithGroundTruth(path_to_ground_truth)\n",
    "#         doc2vec.SaveLinks(grtruth = True)\n",
    "#         #TODO find logic to LoadLink properly and display what is needed\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-30 00:18:57,763 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-10-30 00:18:57,779 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-10-30 00:18:57,781 : INFO : loading Word2Vec object from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model\n",
      "2020-10-30 00:18:57,942 : INFO : loading wv recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model.wv.* with mmap=None\n",
      "2020-10-30 00:18:57,945 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-10-30 00:18:57,948 : INFO : loading vocabulary recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-10-30 00:18:57,950 : INFO : loading trainables recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-10-30 00:18:57,953 : INFO : setting ignored attribute cum_table to None\n",
      "2020-10-30 00:18:57,956 : INFO : loaded c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model\n",
      "2020-10-30 00:18:57,987 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-10-30 00:18:57,993 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x00000190A0E5A340>\n",
      "2020-10-30 00:18:57,997 : INFO : iterating over columns in dictionary order\n",
      "2020-10-30 00:18:58,001 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-10-30 00:18:58,368 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-10-30 00:18:58,530 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-10-30 00:18:58,543 : INFO : Removed 58 and 2468 OOV words from document 1 and 2 (respectively).\n",
      "2020-10-30 00:18:58,546 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-10-30 00:18:58,547 : INFO : built Dictionary(97 unique tokens: ['access', 'can', 'transport', 'via', '\"\");']...) from 2 documents (total 423 corpus positions)\n",
      "2020-10-30 00:18:58,566 : INFO : Computed distances or similarities ('source', 'target')[[0.5401013460499512, 0.6493079189657213]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5401013460499512, 0.6493079189657213)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = open('test_data/RQ11.txt', 'r').read()\n",
    "str2 = open('test_data/us4020.c', 'r').read()\n",
    "TraceLinkValue(str1,str2,\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = (0.5401013460499512, 0.6493079189657213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-26 16:23:20,273 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-10-26 16:23:20,281 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-10-26 16:23:20,282 : INFO : loading Word2Vec object from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2020-10-26 16:23:20,402 : INFO : loading wv recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.* with mmap=None\n",
      "2020-10-26 16:23:20,402 : INFO : loading vectors from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.vectors.npy with mmap=None\n",
      "2020-10-26 16:23:20,438 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-10-26 16:23:20,439 : INFO : loading vocabulary recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.vocabulary.* with mmap=None\n",
      "2020-10-26 16:23:20,440 : INFO : loading trainables recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.* with mmap=None\n",
      "2020-10-26 16:23:20,441 : INFO : loading syn1neg from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.syn1neg.npy with mmap=None\n",
      "2020-10-26 16:23:20,477 : INFO : setting ignored attribute cum_table to None\n",
      "2020-10-26 16:23:20,478 : INFO : loaded c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2020-10-26 16:23:20,555 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-10-26 16:23:20,619 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x0000027180729FA0>\n",
      "2020-10-26 16:23:20,620 : INFO : iterating over columns in dictionary order\n",
      "2020-10-26 16:23:20,624 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-10-26 16:23:22,541 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.347548% density, 0.585365% projected density)\n",
      "2020-10-26 16:23:23,393 : INFO : constructed a sparse term similarity matrix with 0.418460% density\n",
      "2020-10-26 16:23:23,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-10-26 16:23:23,408 : INFO : built Dictionary(3 unique tokens: ['a', 'b', 'c']) from 2 documents (total 6 corpus positions)\n",
      "2020-10-26 16:23:23,409 : INFO : Computed distances or similarities ('source', 'target')[[0.39228455395702855, 0.7182439804836505]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FacadeTest\n",
    "#Test case for proto TLV\n",
    "\n",
    "#Prototype should print a value between 0 or 1. Input values aren't used\n",
    "testTLV = TraceLinkValue(\"a a b\", \"a b c\", \"word2vec\")\n",
    "isinstance(testTLV,tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def NumDoc(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return a list containing the the difference between the two files\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    difference = source_doc - target_doc\n",
    "    return [source_doc, target_doc, difference, -difference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "NumDoc(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Proto NumDoc\n",
    "\n",
    "#Prototype should print an array based on the randomly generated numbers. Input values aren't used.\n",
    "datas1 = [\"Form does not differ from the void, and the void does not differ from the form.\"]\n",
    "datas2 = [\"Form is the void, and the void is form.\"]\n",
    "df1 = pd.DataFrame(data=datas1,columns=['contents'] )\n",
    "df2 = pd.DataFrame(data=datas2,columns=['contents'])\n",
    "testND = NumDoc(df1,df2)\n",
    "assert(isinstance(testND, list))\n",
    "assert(testND[0] == 1)\n",
    "assert(testND[1] == 1)\n",
    "assert(testND[2] == 0)\n",
    "assert(testND[3] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VocabSize(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return a list containing the the difference between the two files in terms of vocab\n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    source_size = len(source_list[0])\n",
    "    target_size = len(target_list[0])\n",
    "    difference = source_size - target_size\n",
    "    return [source_size, target_size, difference, -difference]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 18, -1, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "VocabSize(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test case for Vocab size\n",
    "datas1 = [\"Form does not differ from the void, and the void does not differ from the form.\"]\n",
    "datas2 = [\"Form is the void, and the void is form.\"]\n",
    "df1 = pd.DataFrame(data=datas1,columns=['contents'] )\n",
    "df2 = pd.DataFrame(data=datas2,columns=['contents'])\n",
    "#Prototype should return an array based on random number values. Inputs aren't used.\n",
    "testVS = VocabSize(df1,df2)\n",
    "assert(isinstance(testVS, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def AverageToken(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return a list containing the the difference between the two files in terms of tokens\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    \n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    \n",
    "    source_total_token = sum(source_list[0].values())\n",
    "    target_total_token = sum(target_list[0].values())\n",
    "\n",
    "    source_token = source_total_token/source_doc\n",
    "    target_token = target_total_token/target_doc\n",
    "    difference = source_token - target_token\n",
    "    return [source_token, target_token, difference, -difference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.333333333333334, 16.0, -0.6666666666666661, 0.6666666666666661]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "AverageToken(df,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for AverageToken\n",
    "\n",
    "#Prototype should return an array/list. Inputs aren't used\n",
    "testAT = AverageToken(df1,df2)\n",
    "assert(isinstance(testAT, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Vocab(artifacts_df):\n",
    "    #we can add a parameter for user to specify the number of most frequent token to return\n",
    "    #param a dataframe of contents that need to be processed\n",
    "    #return a list containing the the difference between the two files in terms of vocab\n",
    "    cnts = preprocess(artifacts_df)\n",
    "    vocab_list = cnts[0].most_common(3)\n",
    "    total = sum(cnts[0].values())\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict[vocab_list[0][0]] = [vocab_list[0][1], vocab_list[0][1]/total]\n",
    "    vocab_dict[vocab_list[1][0]] = [vocab_list[1][1], vocab_list[1][1]/total]\n",
    "    vocab_dict[vocab_list[2][0]] = [vocab_list[2][1], vocab_list[2][1]/total]\n",
    "\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': [23, 0.5],\n",
       " 'this': [2, 0.043478260869565216],\n",
       " 'is': [2, 0.043478260869565216]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "Vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Vocab\n",
    "\n",
    "#Prototype Vocab should return dictionary type. \n",
    "#Will expand to test dictionary values once presets can be generated and tested\n",
    "testVocab = Vocab(df1)\n",
    "\n",
    "assert(isinstance(testVocab,dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #export\n",
    "def VocabShared(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return the similarities of vocab in the files \n",
    "    df = pd.concat([source, target])\n",
    "    return Vocab(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': [52, 0.5],\n",
       " 'this': [5, 0.04807692307692308],\n",
       " 'banana': [5, 0.04807692307692308]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example use\n",
    "d = {'contents': ['this is a content of the source file', 'hello world', 'this this is the content of another source file that need to be processed']}\n",
    "df = pd.DataFrame(data=d)\n",
    "target = {'contents': ['this is a content of the target file', 'i like banana banana banana banana banana', 'this is the content of another target file that need to be processed']}\n",
    "df2 = pd.DataFrame(data=target)\n",
    "VocabShared(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test VocabShared\n",
    "\n",
    "#Prototype should show a dictionary\n",
    "#Test will be expanded to verify frequency verification in two samples\n",
    "testVocabS = VocabShared(\"source\",\"target\")\n",
    "assert(isinstance(testVocabS,dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SharedVocabSize(source, target):\n",
    "    #param source a dataframe of the entire source file  \n",
    "    #param target a dataframe of the entire target file \n",
    "    #return the similarities of vocab sizes in the files \n",
    "    df = pd.concat([source, target])\n",
    "    df_counts = preprocess(df)\n",
    "    shared_size = len(df_counts[0])\n",
    "    return shared_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Shared vocab size\n",
    "\n",
    "#Prototype should return an int\n",
    "testSVS = SharedVocabSize(\"source\",\"target\")\n",
    "assert(testSVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Dummy Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MutualInformation(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the mutual information \n",
    "    mutual_information = random.randint(100,200)\n",
    "    return mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Mutual information\n",
    "\n",
    "#Proto mutual information should get an int\n",
    "testMutualInfo = MutualInformation(\"source\",\"target\")\n",
    "assert(testMutualInfo in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CrossEntropy(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the entropy\n",
    "    cross_entropy = random.randint(100,200)\n",
    "    cross_entropy = get_system_entropy_from_df(source, \"col1\",)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Cross Entropy\n",
    "\n",
    "#Proto cross Entropy should return an int\n",
    "testCrossEntropy = CrossEntropy(\"Sauce\",\"Target\")\n",
    "assert(testCrossEntropy in range(100,200))\n",
    "\n",
    "#Eventually cross entropy will properly implement Entropy calculation and the test will compare a known/predicted entropy\n",
    "# to a calculated value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#export\n",
    "def KLDivergence(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the divergence  \n",
    "    divergence = random.randint(100,200)\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test KLDivergence\n",
    "\n",
    "#Proto KLDivegence will return an random int\n",
    "testKLDiver = KLDivergence(\"source\",\"target\")\n",
    "assert(testKLDiver in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
