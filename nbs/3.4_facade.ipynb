{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp facade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from nbdev.showdoc import *\n",
    "#from ds4se.mgmnt.prep.bpe import *\n",
    "#from ds4se.exp.info import *\n",
    "#from ds4se.desc.stats import *\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "#from ds4se.mining.unsupervised.traceability.eval import *\n",
    "from enum import Enum, unique, auto\n",
    "from ds4se.exp import i\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@unique\n",
    "class LinkType(Enum):\n",
    "    req2tc = auto()\n",
    "    req2src = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These methods already exist and work. They can be called by our repo\n",
    "\n",
    "#export\n",
    "def get_docs(df, spm):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the document\n",
    "    docs = []\n",
    "    for fn in df[\"col\"]:\n",
    "        docs += spm.EncodeAsPieces(fn)\n",
    "    return docs\n",
    "\n",
    "#export\n",
    "def get_counters(docs):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the number of tokens   \n",
    "    doc_cnts = []\n",
    "    cnt = Counter()\n",
    "    for tok in docs:\n",
    "        cnt[tok] += 1 \n",
    "        doc_cnts.append(cnt)\n",
    "    return doc_cnts\n",
    "\n",
    "#export\n",
    "def preprocess(artifacts_df):\n",
    "    spm = sp.SentencePieceProcessor()\n",
    "    output = Path('test_data\\models')\n",
    "    system_name = \"test\"\n",
    "    spm.Load(f\"{output / system_name}.model\")\n",
    "    docs = get_docs(artifacts_df,spm)\n",
    "    cnts = get_counters(docs)\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-18 20:20:54,480 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-10-18 20:20:54,489 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-10-18 20:20:54,490 : INFO : loading Word2Vec object from test_data/models/word2vec_libest.model\n",
      "2020-10-18 20:20:54,548 : INFO : loading wv recursively from test_data/models/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-10-18 20:20:54,549 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-10-18 20:20:54,550 : INFO : loading vocabulary recursively from test_data/models/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-10-18 20:20:54,551 : INFO : loading trainables recursively from test_data/models/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-10-18 20:20:54,552 : INFO : setting ignored attribute cum_table to None\n",
      "2020-10-18 20:20:54,553 : INFO : loaded test_data/models/word2vec_libest.model\n",
      "2020-10-18 20:20:54,564 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-10-18 20:20:54,568 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x00000150816E73A0>\n",
      "2020-10-18 20:20:54,569 : INFO : iterating over columns in dictionary order\n",
      "2020-10-18 20:20:54,572 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-10-18 20:20:54,723 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-10-18 20:20:54,789 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-10-18 20:20:54,799 : INFO : Removed 58 and 2468 OOV words from document 1 and 2 (respectively).\n",
      "2020-10-18 20:20:54,800 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-10-18 20:20:54,802 : INFO : built Dictionary(97 unique tokens: ['access', 'can', 'transport', 'via', '\"\");']...) from 2 documents (total 423 corpus positions)\n",
      "2020-10-18 20:20:54,815 : INFO : Computed distances or similarities ('source', 'target')[[0.5401013460499512, 0.6493079189657213]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5401013460499512, 0.6493079189657213)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = open('test_data/LibEST_semeru_format/requirements/RQ11.txt', 'r').read()\n",
    "str2 = open('test_data/LibEST_semeru_format/test/us4020.c', 'r').read()\n",
    "TraceLinkValue(str1,str2,\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = (0.5401013460499512, 0.6493079189657213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TraceLinkValue(source, target, technique, word2vec_metric = \"WMD\"):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #param technique what tecchnique to use to calculate trace values\n",
    "    #return a trace value between [0,1]\n",
    "    \n",
    "    #TODO make the string saved and then return the path to that string\n",
    "    sourcePath = \"path/to/something\"#this is where we save the passed in source\n",
    "    targetPath = \"./something\"\n",
    "    \n",
    "    value = random.randint(0,1)/100\n",
    "\n",
    "    \n",
    "    if (technique == \"VSM\"):\n",
    "        pass\n",
    "    if (technique == \"LDA\"):\n",
    "        pass\n",
    "    if (technique == \"orthogonal\"):\n",
    "        pass\n",
    "    if (technique == \"LSA\"):\n",
    "        pass\n",
    "    if (technique == \"JS\"):\n",
    "        pass\n",
    "    if (technique == \"word2vec\"):\n",
    "        parameter = {\n",
    "            \"vectorizationType\": VectorizationType.word2vec,\n",
    "            \"linkType\": LinkType.req2tc,\n",
    "            \"system\": 'libest',\n",
    "            \"path_to_trained_model\": 'test_data/models/word2vec_libest.model',\n",
    "            \"source_path\": 'test_data/val.csv',\n",
    "            \"target_path\": 'test_data/val.csv',\n",
    "            \"system_path\": 'test_data/val.csv',\n",
    "            \"saving_path\": 'test_data/',\n",
    "            \"names\": ['Source','Target','Linked?']\n",
    "        }\n",
    "        \n",
    "        source_df = pd.DataFrame({ \"ids\": [\"source\"],  \"text\":[source]})\n",
    "        target_df = pd.DataFrame({ \"ids\": [\"target\"],  \"text\":[target]})\n",
    "        word2vec = Word2VecSeqVect(parameter)\n",
    "        word2vec.df_source = source_df\n",
    "        word2vec.df_target = target_df\n",
    "        links = [(source_df[\"ids\"][0],target_df[\"ids\"][0])]\n",
    "        if (word2vec_metric == \"SCM\"):\n",
    "            computeDistanceMetric = word2vec.computeDistanceMetric(links, metric_list = [DistanceMetric.SCM])\n",
    "        else:\n",
    "            computeDistanceMetric = word2vec.computeDistanceMetric(links, metric_list = [DistanceMetric.WMD])\n",
    "        value = (computeDistanceMetric[0][0][2],computeDistanceMetric[0][0][3])    \n",
    "        \n",
    "    if (technique == \"doc2vec\"):\n",
    "        parameter = {\n",
    "            \"vectorizationType\": VectorizationType.doc2vec,\n",
    "            \"linkType\": LinkType.req2tc,\n",
    "            \"system\": 'libest',\n",
    "            \"path_to_trained_model\": 'test_data/models/doc2vec_libest.model',\n",
    "            \"source_path\": 'test_data/val.csv',\n",
    "            \"target_path\": 'test_data/val.csv',\n",
    "            \"system_path\": 'test_data/val.csv',\n",
    "            \"saving_path\": 'test_data/',\n",
    "            \"names\": ['Source','Target','Linked?']\n",
    "        }\n",
    "        \n",
    "        source_df = pd.DataFrame({ \"ids\": [\"source\"],  \"text\":[source]})\n",
    "        target_df = pd.DataFrame({ \"ids\": [\"target\"],  \"text\":[target]})\n",
    "        doc2vec = Doc2VecSeqVect(params = parameter)\n",
    "        doc2vec.df_source = source_df\n",
    "        doc2vec.df_target = target_df\n",
    "        doc2vec.InferDoc2Vec(steps=200)\n",
    "        table = doc2vec.ComputeDistanceArtifacts( sampling=True, samples = 50, metric_list = [DistanceMetric.EUC] )\n",
    "        value = (table[0][0][2], table[0][0][3])\n",
    "        #The bottom is here for reference -- may not need it\n",
    "#         doc2vec.SaveLinks()\n",
    "#         #will most likely need to change this part need to change this part to a different path\n",
    "#         path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "#         doc2vec.MatchWithGroundTruth(path_to_ground_truth)\n",
    "#         doc2vec.SaveLinks(grtruth = True)\n",
    "#         #TODO find logic to LoadLink properly and display what is needed\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbdev_build_docs\n",
    "# notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorizationType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f76cb1bbabdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Prototype should print a value between 0 or 1. Input values aren't used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtestTLV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTraceLinkValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a a b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a b c\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestTLV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-afa4125dcc38>\u001b[0m in \u001b[0;36mTraceLinkValue\u001b[1;34m(source, target, technique, word2vec_metric)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtechnique\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"word2vec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         parameter = {\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[1;34m\"vectorizationType\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVectorizationType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[1;34m\"linkType\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLinkType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreq2tc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;34m\"system\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'libest'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VectorizationType' is not defined"
     ]
    }
   ],
   "source": [
    "#FacadeTest\n",
    "#Test case for proto TLV\n",
    "\n",
    "#Prototype should print a value between 0 or 1. Input values aren't used\n",
    "testTLV = TraceLinkValue(\"a a b\", \"a b c\", \"word2vec\")\n",
    "isinstance(testTLV,tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#export\n",
    "def NumDoc(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return a list containing the the difference between the two files\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    difference = source_doc - target_doc\n",
    "    return [source_doc, target_doc, difference, -difference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Proto NumDoc\n",
    "\n",
    "#Prototype should print an array based on the randomly generated numbers. Input values aren't used.\n",
    "datas1 = [\"Form does not differ from the void, and the void does not differ from the form.\"]\n",
    "datas2 = [\"Form is the void, and the void is form.\"]\n",
    "df1 = pd.DataFrame(data=datas1,columns=['contents'] )\n",
    "df2 = pd.DataFrame(data=datas2,columns=['contents'])\n",
    "testND = NumDoc(df1,df2)\n",
    "assert(isinstance(testND, list))\n",
    "assert(testND[0] == 1)\n",
    "assert(testND[1] == 1)\n",
    "assert(testND[2] == 0)\n",
    "assert(testND[3] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "#export\n",
    "def VocabSize(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return a list containing the the difference between the two files in terms of vocab\n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    source_size = len(source_list[0])\n",
    "    target_size = len(target_list[0])\n",
    "    difference = source_size - target_size\n",
    "    return [source_size, target_size, difference, -difference]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test case for Vocab size\n",
    "datas1 = [\"Form does not differ from the void, and the void does not differ from the form.\"]\n",
    "datas2 = [\"Form is the void, and the void is form.\"]\n",
    "df1 = pd.DataFrame(data=datas1,columns=['contents'] )\n",
    "df2 = pd.DataFrame(data=datas2,columns=['contents'])\n",
    "#Prototype should return an array based on random number values. Inputs aren't used.\n",
    "testVS = VocabSize(df1,df2)\n",
    "assert(isinstance(testVS, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#export\n",
    "def AverageToken(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return a list containing the the difference between the two files in terms of tokens\n",
    "    source_doc = source.shape[0]\n",
    "    target_doc = target.shape[0]\n",
    "    \n",
    "    source_list = preprocess(source)\n",
    "    target_list = preprocess(target)\n",
    "    \n",
    "    source_total_token = sum(source_list[0].values())\n",
    "    target_total_token = sum(target_list[0].values())\n",
    "\n",
    "    source_token = source_total_token/source_doc\n",
    "    target_token = target_total_token/target_doc\n",
    "    difference = source_token - target_token\n",
    "    return [source_token, target_token, difference, -difference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for AverageToken\n",
    "\n",
    "#Prototype should return an array/list. Inputs aren't used\n",
    "testAT = AverageToken(df1,df2)\n",
    "assert(isinstance(testAT, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#export\n",
    "def Vocab(artifacts_df):\n",
    "    #we can add a parameter for user to specify the number of most frequent token to return\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return a list containing the the difference between the two files in terms of vocab\n",
    "    cnts = preprocess(artifacts_df)\n",
    "    vocab_list = cnts[0].most_common(3)\n",
    "    total = sum(cnts[0].values())\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict[vocab_list[0][0]] = [vocab_list[0][1], vocab_list[0][1]/total]\n",
    "    vocab_dict[vocab_list[1][0]] = [vocab_list[1][1], vocab_list[1][1]/total]\n",
    "    vocab_dict[vocab_list[2][0]] = [vocab_list[2][1], vocab_list[2][1]/total]\n",
    "\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Case for Vocab\n",
    "\n",
    "#Prototype Vocab should return dictionary type. \n",
    "#Will expand to test dictionary values once presets can be generated and tested\n",
    "testVocab = Vocab(df1)\n",
    "\n",
    "assert(isinstance(testVocab,dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "#export\n",
    "def VocabShared(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the similarities of vocab in the files \n",
    "    df = pd.concat([source, target])\n",
    "    return Vocab(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test VocabShared\n",
    "\n",
    "#Prototype should show a dictionary\n",
    "#Test will be expanded to verify frequency verification in two samples\n",
    "testVocabS = VocabShared(\"source\",\"target\")\n",
    "assert(isinstance(testVocabS,dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#export\n",
    "def SharedVocabSize(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the similarities of vocab sizes in the files \n",
    "    df = pd.concat([source, target])\n",
    "    df_counts = preprocess(df)\n",
    "    shared_size = len(df_counts[0])\n",
    "    return shared_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Shared vocab size\n",
    "\n",
    "#Prototype should return an int\n",
    "testSVS = SharedVocabSize(\"source\",\"target\")\n",
    "assert(testSVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Dummy Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#export\n",
    "def MutualInformation(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the mutual information \n",
    "    mutual_information = random.randint(100,200)\n",
    "    return mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#TEst Mutual information\n",
    "\n",
    "#Proto mutual information should get an int\n",
    "testMutualInfo = MutualInformation(\"source\",\"target\")\n",
    "assert(testMutualInfo in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#export\n",
    "def CrossEntropy(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the entropy\n",
    "    cross_entropy = random.randint(100,200)\n",
    "    cross_entropy = get_system_entropy_from_df(source, \"col1\",)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test Cross Entropy\n",
    "\n",
    "#Proto cross Entropy should return an int\n",
    "testCrossEntropy = CrossEntropy(\"Sauce\",\"Target\")\n",
    "assert(testCrossEntropy in range(100,200))\n",
    "\n",
    "#Eventually cross entropy will properly implement Entropy calculation and the test will compare a known/predicted entropy\n",
    "# to a calculated value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#export\n",
    "def KLDivergence(source, target):\n",
    "    #param source a string of the entire source file  \n",
    "    #param target a string of the entire target file \n",
    "    #return the divergence  \n",
    "    divergence = random.randint(100,200)\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FacadeTest\n",
    "#Test KLDivergence\n",
    "\n",
    "#Proto KLDivegence will return an random int\n",
    "testKLDiver = KLDivergence(\"source\",\"target\")\n",
    "assert(testKLDiver in range(100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
