{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep.bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Preprocessing\n",
    "\n",
    "> This module comprises all preprocessing techniques applied to software artifacts:\n",
    ">\n",
    ">> Text-based Artifacts: Classical preprocessing (stemming, lemas, etc) and BPE Binary Artifacts:\n",
    ">\n",
    ">> To Do Vision-based Artifacts:\n",
    ">\n",
    ">> To Do Parsing: Techniques to control and manipulate source code (complete with deep generator project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastprogress\n",
      "  Downloading https://files.pythonhosted.org/packages/41/67/347d73405b8612e436a4278f577186a8b783fe757df549ba1a82a2986727/fastprogress-0.2.2-py3-none-any.whl\n",
      "Installing collected packages: fastprogress\n",
      "Successfully installed fastprogress-0.2.2\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install fastprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import random\n",
    "import sentencepiece as sp\n",
    "\n",
    "from fastprogress.fastprogress import master_bar\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f,\n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_dfs(path):\n",
    "    \"\"\"\n",
    "        Grabs the different data splits and converts them into dataframes.\n",
    "        Expects format from Code Search Net Challenge.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path/split).glob(\"**/*.gz\"))\n",
    "        df = jsonl_list_to_dataframe(files, [\"code\", \"docstring\"])\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/tf/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protected final void bindIndexed(Configuration...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public void setServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void addServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public void setServletNames(Collection&lt;String&gt;...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public void addServletNames(String... servletN...</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  protected final void bindIndexed(Configuration...   \n",
       "1  public void setServletRegistrationBeans(\\n\\t\\t...   \n",
       "2  public void addServletRegistrationBeans(\\n\\t\\t...   \n",
       "3  public void setServletNames(Collection<String>...   \n",
       "4  public void addServletNames(String... servletN...   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn, df_val, df_tst = get_dfs(path/\"java/final/jsonl\")\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some test data\n",
    "df_trn.sample(frac = 0.01).to_csv('./test_data/trn.csv', index = False)\n",
    "df_val.sample(frac = 0.01).to_csv('./test_data/val.csv', index = False)\n",
    "df_tst.sample(frac = 0.01).to_csv('./test_data/tst.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def df_to_txt_file(df, output, cols):\n",
    "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
    "    if cols is None: cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "    \n",
    "    with open(output/'text.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(merged_df)))\n",
    "    return output/'text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_df(df, output, model_name, cols = None):\n",
    "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output / model_name} --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_glob(path, glob, model_name):\n",
    "    fns = list(path.glob(glob))\n",
    "    fns = \",\".join(map(str, fns))\n",
    "    sp.SentencePieceTrainer.train(f'--input={fns} --model_prefix={path / model_name} --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_hugface_model(df, output, tokenizer = ByteLevelBPETokenizer(), vocab_sz = 30_000, min_freq = 3, cols = None):\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    tokenizer.train(files = [str(fname)], vocab_size = vocab_sz, min_frequency = min_freq, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./test_data\")\n",
    "model_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private static void createCode(String packageN...</td>\n",
       "      <td>Create the Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Override\\n    public void flushCache() {\\n   ...</td>\n",
       "      <td>LI3492-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void addRule(IntDependency dependency, ...</td>\n",
       "      <td>Add this dependency with the given count to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Override\\n  public boolean removeIfEquals(K k...</td>\n",
       "      <td>Remove the object from the cache.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public void marshall(DatasetContentDeliveryDes...</td>\n",
       "      <td>Marshall the given parameter object.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  private static void createCode(String packageN...   \n",
       "1  @Override\\n    public void flushCache() {\\n   ...   \n",
       "2  public void addRule(IntDependency dependency, ...   \n",
       "3  @Override\\n  public boolean removeIfEquals(K k...   \n",
       "4  public void marshall(DatasetContentDeliveryDes...   \n",
       "\n",
       "                                           docstring  \n",
       "0                                    Create the Java  \n",
       "1                                           LI3492-2  \n",
       "2  Add this dependency with the given count to th...  \n",
       "3                  Remove the object from the cache.  \n",
       "4               Marshall the given parameter object.  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path / 'trn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gen_hugface_model(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'public', 'Ġstatic', 'Ġvoid', 'Ġmain', '(', 'String', '[]', 'Ġargs', ')', 'Ġ{', 'Ġget', 'Dir', 'From', 'Lib', '();', 'Ġ}', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"public static void main(String[] args) { getDirFromLib(); }\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_data/java_tokenizer-vocab.json', 'test_data/java_tokenizer-merges.txt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(str(path), \"java_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = {\n",
    "        'first': ['1', '2', '6', '7', '8'],\n",
    "        'second': ['K', 'M', 'O', 'Q', 'S'],\n",
    "        'third': ['L', 'N', 'P', 'R', 'T']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>K</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>O</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Q</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id Feature1 Feature2\n",
       "0  1        K        L\n",
       "1  2        M        N\n",
       "2  6        O        P\n",
       "3  7        Q        R\n",
       "4  8        S        T"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dummy_data2); df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('test_data/text.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_txt_file(df, Path('./test_data'), list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./test_data\")\n",
    "model_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model_from_dfs(df, path, model_name, list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(str(path/f\"{model_name}.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'Hello,', '▁', 'world!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.EncodeAsPieces(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize_fns(fns, tokenizer, exts, output, data_type):\n",
    "    docs = []\n",
    "    for fn in fns:\n",
    "        system = fn.parent.name\n",
    "        output_path = output/system/data_type\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        files = []\n",
    "        for ext in exts:\n",
    "            files.extend(fn.glob(f'**/*.{ext}'))\n",
    "        for file in files:\n",
    "            if 'README' not in file.name:\n",
    "                with open(file, encoding='ISO-8859-1') as f:\n",
    "                    docs.append(tokenizer.EncodeAsPieces(f.read()))\n",
    "                with open((output_path/file.name).with_suffix('.bpe'), 'w') as f:\n",
    "                    f.write(' '.join(docs[-1]))\n",
    "            \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_bpe_files(path):\n",
    "    bpe_files = []\n",
    "    for file in path.glob('**/*.bpe'):\n",
    "        with open(file) as f:\n",
    "            bpe_files.append(f.read().split(' '))\n",
    "    \n",
    "    return bpe_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def split_lines_to_files(lines, fn_pattern, output_path, tokenizer):\n",
    "    for line in lines:\n",
    "        fn, content = line.split(fn_pattern)\n",
    "        fn = fn.replace('\"', '')\n",
    "        fn = fn.replace(' Test ', '')\n",
    "        content = tokenizer.EncodeAsPieces(content)\n",
    "        with open((output_path/fn).with_suffix('.bpe'), 'w') as f:\n",
    "                    f.write(' '.join(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../benchmarking/traceability/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(str(path/'datasets/italian/italian_bpe.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../benchmarking/traceability/datasets/italian/ebt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebt_path = path/'datasets/italian/ebt'; ebt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ebt_path/'[ebt-raw-req].txt') as f:\n",
    "    split_lines_to_files(f.read().split('\\n')[:-1], '\\t', path/'testbeds/bpe/italian/ebt/req', spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ebt_path/'[ebt-raw-tc].txt') as f:\n",
    "    split_lines_to_files(f.read().split('\\n')[:-1], 'case:', path/'testbeds/bpe/italian/ebt/tc', spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ground_truth(path, language):\n",
    "    all_links = pd.DataFrame([], columns = [\n",
    "        'sys', 'from_type', 'to_type', 'from_file', 'to_file', 'from_doc', 'to_doc'\n",
    "    ])\n",
    "    for fn in path.glob('*.txt'):\n",
    "        content = str(fn.name).split('.')[0][1:-1]\n",
    "        content = content.split('-')\n",
    "        \n",
    "        sys, from_type, to_type = content[0], content[2], content[4]\n",
    "        \n",
    "        with open(fn) as f:\n",
    "            links = f.read().split('\\n')[:-1]\n",
    "            \n",
    "        for link in links:\n",
    "            link = link.split(' ')\n",
    "            root, children = link[0], link[1:]\n",
    "            root = Path(root).with_suffix('.bpe').name\n",
    "            with open(path.parent.parent/'bpe'/language/sys/from_type/root) as f:\n",
    "                root_content = f.read().split(' ')\n",
    "            children = [Path(child).with_suffix('.bpe').name for child in children]\n",
    "            children = [Path('.'.join(str(child).split('.')[-2:])) for child in children]\n",
    "            for child in children:\n",
    "                with open(path.parent.parent/'bpe'/language/sys/to_type/child) as f:\n",
    "                    child_content = f.read().split(' ')\n",
    "                all_links = all_links.append({'sys': sys,\n",
    "                                              'from_type': from_type,\n",
    "                                              'to_type': to_type,\n",
    "                                              'from_file': root,\n",
    "                                              'to_file': str(child),\n",
    "                                              'from_doc': root_content,\n",
    "                                              'to_doc': child_content},\n",
    "                                             ignore_index=True)\n",
    "            \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_non_ground_truth(path, language, gt):\n",
    "    all_non_links = []\n",
    "    \n",
    "    existing_links = ['->'.join(link) for link in zip(gt['from_file'].to_list(), gt['to_file'].to_list())]\n",
    "    bpe_files = list(path.glob('**/*.bpe'))\n",
    "    random.shuffle(bpe_files)\n",
    "    for i in bpe_files[:500]:\n",
    "        sys = i.parent.parent.name\n",
    "        from_type = i.parent.name\n",
    "        if str(from_type) != 'req': continue\n",
    "        with open(i) as f:\n",
    "            i_content = f.read().split(' ')\n",
    "        random.shuffle(bpe_files)\n",
    "        for j in bpe_files[:500]:\n",
    "            if i == j: continue\n",
    "            if '->'.join([i.name, j.name]) in existing_links: continue\n",
    "            to_type = j.parent.name\n",
    "            if str(to_type) == 'req': continue\n",
    "#             if from_type == to_type: continue\n",
    "            with open(j) as f:\n",
    "                j_content = f.read().split(' ')\n",
    "            all_non_links.append([sys, from_type, to_type, i.name, j.name, i_content, j_content])\n",
    "    \n",
    "    all_non_links = pd.DataFrame(all_non_links, columns = [\n",
    "        'sys', 'from_type', 'to_type', 'from_file', 'to_file', 'from_doc', 'to_doc'\n",
    "    ])\n",
    "    return all_non_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_gt_ngt(path, lang):\n",
    "    gt = get_ground_truth(path/'groundtruth'/lang, lang)\n",
    "    ngt = get_non_ground_truth(path/'bpe'/lang, lang, gt)\n",
    "    \n",
    "    return gt, ngt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_mgmnt.prep.i.ipynb.\n",
      "Converted 01_exp.i.ipynb.\n",
      "Converted 02_mgmnt.db.mongo.ipynb.\n",
      "Converted 03_repr.i.ipynb.\n",
      "Converted 04_mining.ir.model.ipynb.\n",
      "Converted 05_mining.ir.i.ipynb.\n",
      "Converted 06_benchmark.traceability.ipynb.\n",
      "Converted 07_repr.roberta.train.ipynb.\n",
      "Converted 08_exp.info.ipynb.\n",
      "Converted 09_desc.stats.ipynb.\n",
      "Converted 10_vis.ipynb.\n",
      "Converted 11_mgmnt.prep.nltk.ipynb.\n",
      "Converted 12_repr.roberta.eval.ipynb.\n",
      "Converted 14_mgmnt.prep.bpe.ipynb.\n",
      "Converted 15_desc.metrics.se.ipynb.\n",
      "Converted 16_repr.word2vec.train.ipynb.\n",
      "Converted 17_repr.doc2vec.train.ipynb.\n",
      "Converted 18_repr.doc2vec.eval.ipynb.\n",
      "Converted 19_repr.word2vec.eval.ipynb.\n",
      "Converted 20_benchmark.codegen.ipynb.\n",
      "Converted 21_inf.i.ipynb.\n",
      "Converted 22_inf.bayesian.ipynb.\n",
      "Converted 23_inf.causal.ipynb.\n",
      "Converted aa_blog.example.ipynb.\n",
      "Converted ab_templates.example.ipynb.\n",
      "Converted ac_emp.eval.pp1.rq1.ipynb.\n",
      "Converted ad_emp.eval.pp1.rq2.ipynb.\n",
      "Converted ae_emp.eval.pp1.rq3.ipynb.\n",
      "Converted af_emp.eval.pp1.rq4.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
