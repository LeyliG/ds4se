{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp desc.metrics.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of your data\n",
    "\n",
    "> This module comprises some of the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
    ">\n",
    "> - Descriptive statistics\n",
    "> - Software Metrics\n",
    "> - Information Theory\n",
    "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
    "> - Inference: Probabilistic and Causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifically in this module\n",
    "\n",
    "> - Cyclomatic complexity (CYCLO)\n",
    "> - Number of lines of code (NLOC)\n",
    "> - Lack of Cohesion of Methods 5 (LCOM5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current limitations\n",
    "\n",
    "> - Can only compute LCOM5 for Java files\n",
    "> - Can theoretically compute Cyclomatic Complexity for \n",
    "> > - C\n",
    "> > - C++ (works with C++14)\n",
    "> > - Java\n",
    "> > - C# (C Sharp)\n",
    "> > - JavaScript (With ES6 and JSX)\n",
    "> > - Objective-C\n",
    "> > - Swift\n",
    "> > - Python\n",
    "> > - Ruby\n",
    "> > - TTCN-3\n",
    "> > - PHP\n",
    "> > - Scala\n",
    "> > - GDScript\n",
    "> > - Golang\n",
    "> > - Lua\n",
    "> > - Rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lizard in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (1.17.4)\n",
      "Collecting tree_sitter\n",
      "  Downloading tree_sitter-0.2.0-cp38-cp38-macosx_10_14_x86_64.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tree-sitter\n",
      "Successfully installed tree-sitter-0.2.0\n",
      "Requirement already satisfied: pandas in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: scipy in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from scipy) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: chardet in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (3.0.4)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/willkinney/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=6776fef7e3f915559391e747103d486efb3d07619b089a43bbd5eb137e7ebef4\n",
      "  Stored in directory: /Users/willkinney/Library/Caches/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install metrics # Outdated Cyclomatic Complexity tool\n",
    "!pip install lizard\n",
    "!pip install tree_sitter\n",
    "\n",
    "# The below modules need to be loaded when not being used in Google Colab\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install chardet\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from numpy import mean, std\n",
    "from statistics import median\n",
    "from scipy.stats import sem, t\n",
    "import lizard\n",
    "import matplotlib.pyplot as plt\n",
    "from tree_sitter import Language, Parser, Node\n",
    "#Decoding files\n",
    "import chardet\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "\n",
    "# TODO: Remove when mongo call is implemented\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tree-sitter-java'...\n",
      "remote: Enumerating objects: 225, done.\u001b[K\n",
      "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
      "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
      "remote: Total 1510 (delta 115), reused 172 (delta 72), pack-reused 1285\u001b[K\n",
      "Receiving objects: 100% (1510/1510), 13.05 MiB | 14.91 MiB/s, done.\n",
      "Resolving deltas: 100% (935/935), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tree-sitter/tree-sitter-java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_unicode(file_path):\n",
    "    \"\"\"Detects file encoding and returns unicode. Inspired by http://reinvantveer.github.io/2017/05/19/unicode-dammit.html\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        detection = chardet.detect(f.read())\n",
    "        \n",
    "    enc = detection[\"encoding\"]\n",
    "    if detection[\"encoding\"] == \"ascii\":\n",
    "        with open(file_path, encoding=\"ascii\") as f:\n",
    "            data = f.read()\n",
    "    elif detection[\"encoding\"] == \"ISO-8859-9\":\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            enc = \"utf-8\"\n",
    "            data = f.read()\n",
    "    else:\n",
    "        try:\n",
    "            # Try to open as non unicode file\n",
    "            with open(file_path, encoding=detection[\"encoding\"]) as f:\n",
    "                data = f.read()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Cannot return dictionary from empty or invalid csv file {file_path} due to {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(f\"Cannot return dictionary from empty or invalid csv file {file_path}\")\n",
    "\n",
    "    return UnicodeDammit(data).unicode_markup, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def simulate_getting_dataframes_from_mongo(folder_path):\n",
    "    \"\"\"Loads files from a specified folder into a pandas dataframe\"\"\"\n",
    "    corpus_data = {\"system\": [], \"name\": [], \"ground_truth\": [], \"contents\": [], \"encoding\": []}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not os.path.isdir(os.path.join(folder_path, file)) and file != \".DS_Store\":\n",
    "            corpus_data[\"system\"].append(None)\n",
    "            corpus_data[\"name\"].append(file)\n",
    "            corpus_data[\"ground_truth\"].append(\"src\")\n",
    "            contents, enc = get_unicode(os.path.join(folder_path, file))\n",
    "            corpus_data['encoding'].append(enc)\n",
    "            corpus_data['contents'].append(contents)\n",
    "    corpus_df = pd.DataFrame(data = corpus_data)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def add_mccabe_metrics(df, data_col, name_col):\n",
    "    \"\"\"Adds information about function length and cyclomatic complexity for classes to a dataframe\"\"\"\n",
    "    num_funcs = []\n",
    "    class_ccn = []\n",
    "    avg_func_ccn = []\n",
    "    avg_func_nloc = []\n",
    "    for i in range(len(df)):\n",
    "        file_num_funcs = []\n",
    "        file_class_ccn = []\n",
    "        file_avg_func_ccn = []\n",
    "        file_avg_func_nloc = []\n",
    "        metrics = lizard.analyze_file.analyze_source_code(df[name_col][i], df[data_col][i])\n",
    "        class_dict = {}\n",
    "        for func in metrics.function_list:\n",
    "            class_name = '::'.join(func.name.split(\"::\")[:-1])\n",
    "            if class_name in class_dict:\n",
    "                class_dict[class_name].append(func)\n",
    "            else:\n",
    "                class_dict[class_name] = [func]\n",
    "        for class_key in class_dict:\n",
    "            total_class_ccn = 0\n",
    "            total_class_nloc = 0\n",
    "            for func in class_dict[class_key]:\n",
    "                total_class_ccn += func.cyclomatic_complexity\n",
    "                total_class_nloc += func.length\n",
    "            file_num_funcs.append(len(class_dict[class_key]))\n",
    "            file_class_ccn.append(total_class_ccn)\n",
    "            file_avg_func_ccn.append(total_class_ccn/len(class_dict[class_key]))\n",
    "            file_avg_func_nloc.append(total_class_nloc/len(class_dict[class_key]))\n",
    "\n",
    "        num_funcs.append(file_num_funcs)\n",
    "        class_ccn.append(file_class_ccn)\n",
    "        avg_func_ccn.append(file_avg_func_ccn)\n",
    "        avg_func_nloc.append(file_avg_func_nloc)\n",
    "\n",
    "\n",
    "    df[\"num_funcs\"] = num_funcs\n",
    "    df[\"class_ccn\"] = class_ccn\n",
    "    df[\"avg_func_ccn\"] = avg_func_ccn\n",
    "    df[\"avg_func_nloc\"] = avg_func_nloc\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_parser_builds(path=None):\n",
    "    \"\"\"Creates a dictionary of tree-sitter parsers for select languages\"\"\"\n",
    "    Language.build_library(\n",
    "        # Store the library in the `build` directory\n",
    "        'build/my-languages.so',\n",
    "\n",
    "        # Include one or more languages\n",
    "        [\n",
    "            'tree-sitter-java'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    JAVA_LANGUAGE = Language('build/my-languages.so', 'java')\n",
    "    \n",
    "    return {\"java\":JAVA_LANGUAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lang_keywords():\n",
    "    \"\"\"Returns language specific keywords for a parser to find\"\"\"\n",
    "    keyword_dict = {}\n",
    "    keyword_dict[\"java\"] = {\"class\": \"class_declaration\", \"method\":\"method_declaration\", \"field_dec\":\"field_declaration\", \"field_name\":\"identifier\"}\n",
    "    return keyword_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_class_nodes(root_node, key):\n",
    "    \"\"\"Recursively searches an AST for class nodes\"\"\"\n",
    "    node_list = []\n",
    "    def rec_class_search(node):\n",
    "        if node.type == key[\"class\"]:\n",
    "            node_list.append(node)\n",
    "        for child in node.children:\n",
    "            rec_class_search(child)\n",
    "\n",
    "    rec_class_search(root_node)\n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_method_nodes(class_node, key):\n",
    "    \"\"\"Recursively searches an AST for method nodes\"\"\"\n",
    "    node_list = []\n",
    "    def rec_method_search(node):\n",
    "        if node.type == key[\"method\"]:\n",
    "            node_list.append(node)\n",
    "        if node.type != key[\"class\"]:\n",
    "            for child in node.children:\n",
    "                rec_method_search(child)\n",
    "    \n",
    "    for node in class_node.children:\n",
    "        rec_method_search(node)\n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_field_names(class_node, file_bytes, key):\n",
    "    \"\"\"Finds the fields/attributes for a class AST\"\"\"\n",
    "    class_fields = []\n",
    "    \n",
    "    def rec_name_search(node):\n",
    "        if node.type == key[\"field_name\"]:\n",
    "            word = []\n",
    "            for i in range(node.start_byte, node.end_byte):\n",
    "                word.append(file_bytes[i])\n",
    "            class_fields.append(word)\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                rec_name_search(child)\n",
    "    \n",
    "    def rec_field_search(node):\n",
    "        if node.type == key[\"field_dec\"]:\n",
    "            rec_name_search(node)\n",
    "        if node.type != key[\"class\"]:\n",
    "            for child in node.children:\n",
    "                rec_field_search(child)\n",
    "\n",
    "    for node in class_node.children:\n",
    "        rec_field_search(node)\n",
    "\n",
    "    return class_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_string_in_text(node, pattern, file_bytes):\n",
    "    \"\"\"Counts the number of occurences of a byte-pattern array in a sample of code\"\"\"\n",
    "    if len(node.children) > 0:\n",
    "        count = 0\n",
    "        for i in node.children:\n",
    "            count += find_string_in_text(i, pattern, file_bytes)\n",
    "        return count\n",
    "    else:\n",
    "        word = []\n",
    "        for i in range(node.start_byte, node.end_byte):\n",
    "            num_index_fails = 0\n",
    "            try:\n",
    "                word.append(file_bytes[i])\n",
    "            except IndexError:\n",
    "                num_index_fails += 1\n",
    "        if(num_index_fails):\n",
    "            print(f\"INDEX ERROR ({num_index_fails} times)\")\n",
    "            print(\"Start byte:\", node.start_byte, \"End byte:\", node.end_byte, \"Word:\", word)\n",
    "        if word == pattern:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def distinct_field_calls(class_node, field_names, file_bytes, key):\n",
    "    \"\"\"Recursively searches an AST for method nodes\"\"\"\n",
    "    total_distinct_calls = []\n",
    "    \n",
    "    def rec_method_search(node):\n",
    "        if node.type == key[\"method\"]:\n",
    "            distinct_method_field_calls = 0\n",
    "            for field in field_names:\n",
    "                if find_string_in_text(node, field, file_bytes):\n",
    "                    distinct_method_field_calls += 1\n",
    "            total_distinct_calls.append(distinct_method_field_calls)\n",
    "        if node.type != key[\"class\"]:\n",
    "            for child in node.children:\n",
    "                rec_method_search(child)\n",
    "    \n",
    "    for node in class_node.children:\n",
    "        rec_method_search(node)\n",
    "        \n",
    "    return len(total_distinct_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_lcom5(tree, extension, file_bytes, name):\n",
    "    \"\"\"Parses the syntax tree of code to calculate the LCOM5 of its classes\"\"\"\n",
    "    keyword_dict = lang_keywords()\n",
    "\n",
    "    if extension not in keyword_dict:\n",
    "        print(f\"Tried to get LCOM5 of file with unsupported extension '.{extension}', 0 assigned to column.\")\n",
    "        return [\"Undefined\"]\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    keywords = keyword_dict[extension]\n",
    "    class_nodes = find_class_nodes(root_node, keywords)\n",
    "    class_method_nodes = []\n",
    "    class_field_names = []\n",
    "    class_dfc = [] # Distinct field calls, as per the definition of LCOM5\n",
    "    for node in enumerate(class_nodes):\n",
    "        class_method_nodes.append(find_method_nodes(node[1], keywords))\n",
    "        class_field_names.append(find_field_names(node[1], file_bytes, keywords))\n",
    "        class_dfc.append(distinct_field_calls(node[1], class_field_names[node[0]], file_bytes, keywords))\n",
    "    lcom5_list = []\n",
    "    for j in range(len(class_nodes)):\n",
    "        num_fields = len(class_field_names[j])\n",
    "        num_meths = len(class_method_nodes[j])\n",
    "        num_dac = class_dfc[j]\n",
    "        numerator = num_dac - (num_meths*num_fields)\n",
    "        denominator = num_fields - (num_meths*num_fields)\n",
    "        if denominator == 0:\n",
    "            lcom5_list.append(\"Undefined\")\n",
    "        else:\n",
    "            lcom5_list.append(numerator/denominator)\n",
    "    return lcom5_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_lcom5(df, col):\n",
    "    \"\"\"Adds a column with the LCOM5 of each class of each file to a dataframe\"\"\"\n",
    "    lang_builds = create_parser_builds()\n",
    "    parser = Parser()\n",
    "    class_lcom5 = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        ext = df[\"name\"][i].split('.')[-1]\n",
    "        parser.set_language(lang_builds[ext])\n",
    "        enc = df[\"encoding\"][i]\n",
    "        tree = parser.parse(bytes(df[\"contents\"][i], df[\"encoding\"][i]))\n",
    "        class_lcom5.append(calculate_lcom5(tree, ext, bytes(df[\"contents\"][i], df[\"encoding\"][i]), df[\"name\"][i]))\n",
    "    df[\"class_lcom5\"] = class_lcom5\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def flatten_lol(list_list):\n",
    "    \"\"\"Takes in a list of lists and flattens it, returning a list of each entry\"\"\"\n",
    "    flattened_list = []\n",
    "    for sublist in list_list:\n",
    "        for entry in sublist:\n",
    "            flattened_list.append(entry)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def display_numeric_col_stats(col, conf = 0.95, sig_figs = 4, clean=True, verbose_clean=False):\n",
    "    \"\"\"Computes statistical metrics about the entries in a dataframe column or list\"\"\"\n",
    "    previous_length = len(col)\n",
    "    numeric_types = [int, float, complex]\n",
    "    if clean: col = [x for x in col if type(x) in numeric_types]\n",
    "    if verbose_clean: print(f\"Cleaning removed {previous_length - len(col)} non-numeric entries\")\n",
    "\n",
    "    if len(col) < 1:\n",
    "        print(\"Error, data must contain at least one valid entry to display statistics\")\n",
    "        return\n",
    "\n",
    "    print(\"Min =\", round(min(col), sig_figs))\n",
    "    print(\"Max =\", round(max(col), sig_figs))\n",
    "    print(\"Average =\", round(mean(col), sig_figs))\n",
    "    print(\"Median =\", round(median(col), sig_figs))\n",
    "    print(\"Standard Deviation =\", round(std(col), sig_figs))\n",
    "    \n",
    "    n = len(col)\n",
    "    m = mean(col)\n",
    "    std_err = sem(col)\n",
    "    h = std_err * t.ppf((1 + conf) / 2, n - 1)\n",
    "\n",
    "    start = m - h\n",
    "    end = m + h\n",
    "    print(f\"{conf} of data points fall between {round(start, sig_figs)} and {round(end, sig_figs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def display_numeric_col_hist(col, col_name=\"Metric\", num_bins=20, clean=True, verbose_clean=False):\n",
    "    \"\"\"Displays a histogram with a customized number of bins for the data in a specified dataframe column or list\"\"\"\n",
    "    previous_length = len(col)\n",
    "    numeric_types = [int, float, complex]\n",
    "    if clean: col = [x for x in col if type(x) in numeric_types]\n",
    "    if verbose_clean: print(f\"Cleaning removed {previous_length - len(col)} non-numeric entries\")\n",
    "\n",
    "    if len(col) < 1:\n",
    "        print(\"Error, data must contain at least one valid entry to display histogram\")\n",
    "        return    \n",
    "\n",
    "    rng = max(col) - min(col)\n",
    "    num = len(col)\n",
    "    stnd_dev = std(col)\n",
    "\n",
    "    plt.hist(col, num_bins, color=\"blue\", alpha=0.5, edgecolor=\"black\", linewidth=1.0)\n",
    "    plt.title(col_name + \" Histogram\")\n",
    "    plt.ylabel(\"Value  Range  Occurrences\")\n",
    "    plt.xlabel(col_name)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
