{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exp.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of your data\n",
    "\n",
    "> This module comprises all the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
    ">\n",
    "> - Descriptive statistics\n",
    "> - Software Metrics\n",
    "> - Information Theory\n",
    "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
    "> - Inference: Probabilistic and Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "import dit\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import sem, t\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# TODO: Remove when mongo call is implemented\n",
    "import os"
   ]
  },
  {
<<<<<<< Updated upstream
=======
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
>>>>>>> Stashed changes
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVED NECESSARY PIECES TO 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with actual mongo call\n",
    "def simulate_getting_dataframes_from_mongo():\n",
    "    corpus_data = {'file_name': [], 'data_type': [], 'contents': []}\n",
    "    path = \"./requirements\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('req')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./source_code\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('src')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./tests\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('test')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    corpus_df = pd.DataFrame(data = corpus_data)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def df_to_txt_file(df, output, cols):\n",
    "#     \"\"\"Converts a dataframe into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
    "#     if cols is None: cols = list(df.columns)\n",
    "#     merged_df = pd.concat([df[col] for col in cols])\n",
    "    \n",
    "#     with open(output + '_text.txt', 'w') as f:\n",
    "#         f.write('\\n'.join(list(merged_df)))\n",
    "#     return output + '_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def gen_sp_model(df, output, model_name, cols=None):\n",
    "#     \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
    "#     fname = df_to_txt_file(df, output, cols)\n",
    "#     sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output + model_name} --hard_vocab_limit=false --model_type=bpe')\n",
    "#     return output + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_text(text, model_prefix):\n",
    "    '''Encodes text using a pre-trained sp model, returns the occurrences of each token in the text'''\n",
    "    sp_processor = sp.SentencePieceProcessor()\n",
    "    sp_processor.Load(f\"{model_prefix}.model\")\n",
    "    token_counts = Counter()\n",
    "    encoding = sp_processor.encode_as_pieces(text)\n",
    "    for piece in encoding:\n",
    "        token_counts[piece] += 1\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dit_shannon(token_counts):\n",
    "    '''Takes in a counter object of token occurrences, computes the entropy of the corpus that produced it'''\n",
    "    num_tokens = 0\n",
    "    for token in token_counts:\n",
    "        num_tokens += token_counts[token]\n",
    "    outcomes = list(set(token_counts.elements()))\n",
    "    frequencies = []\n",
    "    for token in token_counts:\n",
    "        frequencies.append((token_counts[token])/num_tokens)\n",
    "    d = dit.ScalarDistribution(outcomes, frequencies)\n",
    "    return dit.shannon.entropy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropies_of_df_entries(df, col, model_prefix):\n",
    "    '''Returns a list of the entropies of each entry in a dataframe column'''\n",
    "    entropies = []\n",
    "    for data in df[col]:\n",
    "        token_counts= encode_text(data, model_prefix)\n",
    "        entropies.append(dit_shannon(token_counts))\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish this such that is finds the entropy of the entire corpus \\\n",
    "#       and preserves the individual token frequencies so that we can   \\\n",
    "#      compute the most common tokens\n",
    "\n",
    "# def entropy_of_whole_corpus(df, col, model_prefix):\n",
    "#     '''Returns a dictionary of the entropies of each token in a dataframe corpus'''\n",
    "#     entropies = {}\n",
    "#     token_counts = encode_text(pd.concat[col], model_prefix)\n",
    "#     entropies.append(dit_shannon(token_counts))\n",
    "#     return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do we need this function?\n",
    "import math\n",
    "#export\n",
    "def manual_shannon(token_freqs):\n",
    "    sum = 0\n",
    "    for i in token_freqs:\n",
    "        sum += i * math.log(1/i, 2)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Do we need this function?\n",
    "def sort_token_data(token_data):\n",
    "    return sorted(token_data.items() ,  key=lambda x: x[1][\"Occurrences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBest Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './requirements'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8f81501d02eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a dataframe of the requirements, source code and test case data for LIBest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Create a sentencepiece model using the entire LIBest corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mLIB_corpus_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_getting_dataframes_from_mongo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mLIB_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_sp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_corpus_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LIBest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_sp_bpe_modal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-9e54c134148d>\u001b[0m in \u001b[0;36msimulate_getting_dataframes_from_mongo\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorpus_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'contents'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./requirements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mcorpus_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcorpus_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'req'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './requirements'"
     ]
    }
   ],
   "source": [
    "# Create a dataframe of the requirements, source code and test case data for LIBest\n",
    "# Create a sentencepiece model using the entire LIBest corpus\n",
    "LIB_corpus_df = simulate_getting_dataframes_from_mongo()\n",
    "LIB_model = gen_sp_model(LIB_corpus_df, output='LIBest', model_name='_sp_bpe_modal', cols=['contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LIB_corpus_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ad52f8184474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the model to compute each file's entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLIB_entropies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropies_of_df_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_corpus_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'contents'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLIB_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'LIB_corpus_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the model to compute each file's entropy\n",
    "LIB_entropies = entropies_of_df_entries(LIB_corpus_df, 'contents', LIB_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LIB_entropies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-39667ddddcce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate metrics on the LIBest corpus entropies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max entropy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_entropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Min entropy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_entropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average entropy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_entropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Median entropy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_entropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LIB_entropies' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate metrics on the LIBest corpus entropies\n",
    "print(\"Max entropy:\", max(LIB_entropies))\n",
    "print(\"Min entropy:\", min(LIB_entropies))\n",
    "print(\"Average entropy:\", mean(LIB_entropies))\n",
    "print(\"Median entropy:\", stat.median(LIB_entropies))\n",
    "\n",
    "print(\"Entropy Standard Deviation:\", std(LIB_entropies))\n",
    "\n",
    "confidence = 0.95\n",
    "n = len(LIB_entropies)\n",
    "m = mean(LIB_entropies)\n",
    "std_err = sem(LIB_entropies)\n",
    "h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "start = m - h\n",
    "end = m + h\n",
    "print(f\"95% of entropies fall within {start} and {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LIB_entropies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-02881aea3d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a histogram of the entropy distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_entropies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num Files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LIB_entropies' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a histogram of the entropy distribution\n",
    "plt.hist(LIB_entropies, bins = 20)\n",
    "plt.ylabel(\"Num Files\")\n",
    "plt.xlabel(\"Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Corpus as a Whole"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Code (Testing)"
   ]
=======
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ooooo', 'hi']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tok_counts = Counter()\n",
    "tok_counts['hi'] += 1\n",
    "tok_counts['ooooo'] += 1\n",
    "tok_counts['ooooo'] += 1\n",
    "# print(p)\n",
    "# print(len(p))\n",
    "# for i in p.elements():\n",
    "#   print(i)\n",
    "print(list(set(tok_counts.elements())))\n",
    "\n",
    "\n",
    "\n",
    "num_tokens = 3\n",
    "frequencies = [count/num_tokens for token in outcomes for count in ]"
   ]
=======
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9219280948873623\n",
      "0.9219280948873625\n"
     ]
    }
   ],
   "source": [
    "freq = [.8, .1, .1]\n",
    "# toks = [str(i) for i in range(len(freq))]\n",
    "toks = ('a', 'bb', 'ccc')\n",
    "\n",
    "d = dit.ScalarDistribution(toks, freq)\n",
    "print(dit.shannon.entropy(d))\n",
    "print(manual_shannon(freq))\n",
    "\n"
   ]
=======
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "0    4\n",
      "1    5\n",
      "2    6\n",
      "0    7\n",
      "1    8\n",
      "2    9\n",
      "Name: contents, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a_corpus = {'file_name': [\"aa\", \"ab\", \"ac\"], 'contents': [1, 2, 3]}\n",
    "a_df = pd.DataFrame(data = a_corpus)\n",
    "b_corpus = {'file_name': [\"ba\", \"bb\", \"bc\"], 'contents': [4, 5, 6]}\n",
    "b_df = pd.DataFrame(data = b_corpus)\n",
    "c_corpus = {'file_name': [\"ca\", \"cb\", \"cc\"], 'contents': [7, 8, 9]}\n",
    "c_df = pd.DataFrame(data = c_corpus)\n",
    "\n",
    "corpus_data = {'a':a_df, 'b':b_df, 'd':c_df}\n",
    "\n",
    "# corpus_contents = pd.Series([])\n",
    "corpus_contents = []\n",
    "for data_type in corpus_data.keys():\n",
    "    corpus_contents.append(corpus_data[data_type].contents)\n",
    "print(pd.concat(corpus_contents))\n",
    "\n",
    "\n",
    "# print([corpus_data[i] for i in corpus_data.keys()])\n",
    "\n",
    "# merged_corpus = pd.concat([df.contents for df in corpus_data[data_type] for data_type in corpus_data.keys()])\n",
    "# merged_corpus = pd.concat([df for data_type in corpus_data.keys() for df in corpus_data[data_type].contents])\n",
    "# flatten_matrix = [val for sublist in matrix for val in sublist] \n",
    "\n",
    "# something = [df[\"contents\"] for data_type in corpus_data.keys() for df in corpus_data[data_type]]\n",
    "# print(something)\n",
    "# print(pd.concat(something))\n",
    "# print([i for i in range(10)])\n",
    "\n",
    "# print(merged_df)\n",
    "# print(pd.concat([a_df.contents, b_df.contents]))"
   ]
=======
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< Updated upstream
   "source": [
    "#Rank the system/datasets according to the confidence intervals\n",
    "#Compute the confidence intervals for all cross-entropy values\n",
    "#Rank the systems/datasets according to cross-entropy values\n",
    "#Top 50 most frequent tokens of each system and corpus (one system has generally two corpora)\n",
    "#Top 50 least frequent tokes of each system and corpus\n",
    "#What are the tokens that are in the target and not in the source (and the other way around)? Compute the distribution for those tokens\n",
    "#What are the mutual tokens (source and target)? please compute distribution\n",
    "#-Compute confidence intervals for the software metrics on source code (e.g., cyclo, loc, lcom5)\n"
   ]
=======
   "source": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< Updated upstream
   "source": [
    "testing"
   ]
=======
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
