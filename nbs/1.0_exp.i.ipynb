{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exp.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of your data\n",
    "\n",
    "> This module comprises all the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
    ">\n",
    "> - Descriptive statistics\n",
    "> - Software Metrics\n",
    "> - Information Theory\n",
    "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
    "> - Inference: Probabilistic and Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "import dit\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import sem, t\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO: Remove when mongo call is implemented\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simulates getting dataframes from a mongodb. To be replaced with actual mango call.\n",
    ":returns: corpus dataframe \n",
    "'''\n",
    "def simulate_getting_dataframes_from_mongo():\n",
    "    corpus_data = {'file_name': [], 'data_type': [], 'contents': []}\n",
    "    path = \"./test_data/LibEST_semeru_format/requirements\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('req')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./test_data/LibEST_semeru_format/source_code\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('src')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./test_data/LibEST_semeru_format/test\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('test')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    corpus_df = pd.DataFrame(data = corpus_data)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Form does not differ from the void, and the vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Form is the void, and the void is form.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contents\n",
       "0  Form does not differ from the void, and the vo...\n",
       "1            Form is the void, and the void is form."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hide\n",
    "#This isn't actually used so we don't need to test it but here you can see how a data frame is organized\n",
    "#df = simulate_getting_dataframes_from_mongo()\n",
    "datas = [\"Form does not differ from the void, and the void does not differ from the form.\", \"Form is the void, and the void is form.\"]\n",
    "df = pd.DataFrame(data=datas,columns=['contents'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Converts a dataframe into a text file that SentencePiece can use to train a BPE model\n",
    "\n",
    ":param df: dataframe to be converted\n",
    ":param output: beginning of filename (output + _text.txt)\n",
    ":param cols: columns\n",
    ":returns: dataframe as file\n",
    "'''\n",
    "def df_to_txt_file(df, output, cols):\n",
    "    \"\"\"\"\"\"\n",
    "    if cols is None: cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "  \n",
    "    with open(output + '_text.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(merged_df)))\n",
    "    return output + '_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_to_txt_file(df,'alexTest', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "\n",
    "#Create a file\n",
    "df_to_txt_file(df,'output', None)\n",
    "#Is it there?\n",
    "test_bool = os.path.isfile('./output_text.txt')\n",
    "assert(test_bool == True)\n",
    "\n",
    "os.remove('./output_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Trains a SentencePiece BPE model from a pandas dataframe\n",
    "\n",
    ":param df: input dataframe\n",
    ":param output: beginning of filename (output + _text.txt)\n",
    ":param model_name: name of the model\n",
    ":param cols: columns\n",
    ":returns: trained model\n",
    "'''\n",
    "def gen_sp_model(df, output, model_name, cols=None):\n",
    "    \n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output + model_name} --hard_vocab_limit=false --model_type=bpe')\n",
    "    return output + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "\n",
    "#Here we test the gen sp model method to see if it properly creates files\n",
    "actual = gen_sp_model(df,'output','_sp_bpe_modal',cols=[\"contents\"])\n",
    "expected = os.path.isfile('./' + actual + '.model')\n",
    "assert(expected)\n",
    "os.remove('./' + actual + '.model')\n",
    "expected2 = os.path.isfile('./'+ actual + '.vocab')\n",
    "assert(expected2)\n",
    "os.remove('./'+ actual + '.vocab')\n",
    "\n",
    "os.remove('./output_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Encodes text using a pre-trained sp model, returns the occurrences of each token in the text\n",
    "\n",
    ":param text: input text\n",
    ":param model_prefix: prefix for trained sp model (f + model_prefix + .model)\n",
    ":returns: occurrences of each token in text\n",
    "'''\n",
    "def encode_text(text, model_prefix):\n",
    "    sp_processor = sp.SentencePieceProcessor()\n",
    "    sp_processor.Load(f\"{model_prefix}.model\")\n",
    "    token_counts = Counter()\n",
    "    encoding = sp_processor.encode_as_pieces(text)\n",
    "    for piece in encoding:\n",
    "        token_counts[piece] += 1\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "#ExpTest\n",
    "\n",
    "#This tests the encode text method assuming the sp model(?)\n",
    "\n",
    "sp_model = gen_sp_model(df,'output','_sp_bpe_modal',cols=[\"contents\"])\n",
    "\n",
    "actual = encode_text(\"Form does not differ from the void, and the void does not differ from the form.\", 'output_sp_bpe_modal')\n",
    "\n",
    "expected = Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1})\n",
    "\n",
    "assert(actual == expected)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Notes: token_counts is a counter object.\n",
    "'''\n",
    "Takes in a counter object of token occurrences, computes the entropy of the corpus that produced it\n",
    "\n",
    ":param token_counts: counter object of token occurrences\n",
    ":returns: shannon entropy of corpus\n",
    "'''\n",
    "def dit_shannon(token_counts):\n",
    "    num_tokens = 0\n",
    "    for token in token_counts:\n",
    "        num_tokens += token_counts[token]\n",
    "    outcomes = list(set(token_counts.elements()))\n",
    "    frequencies = []\n",
    "    for token in token_counts:\n",
    "        frequencies.append((token_counts[token])/num_tokens)\n",
    "    d = dit.ScalarDistribution(outcomes, frequencies)\n",
    "    return dit.shannon.entropy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6783847516036627\n",
      "3.6783847516036627\n",
      "Counter({' ': 15, 'o': 10, 'd': 7, 'e': 7, 'f': 7, 'r': 6, 't': 5, 'm': 4, 'i': 4, 'n': 3, 'h': 3, 's': 2, 'v': 2, 'F': 1, ',': 1, 'a': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "#ExpTest\n",
    "#This test takes a sample string and creates a counter that it runs dit shannon on to verify the shannon functionality\n",
    "sample_txt = \"Form does not differ from the void, and the void does not differ from the form.\"\n",
    "sample_tcnt = Counter(sample_txt)\n",
    "\n",
    "answer = dit_shannon(sample_tcnt)\n",
    "expected = 3.6783847516036627\n",
    "#assert(answer == expected)\n",
    "print(answer)\n",
    "print(expected)\n",
    "print(sample_tcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Returns a list of the entropies of each entry in a dataframe column\n",
    "\n",
    ":param df: input dataframe\n",
    ":param col: column name from dataframe\n",
    ":param model_prefix: name of pre-trained model\n",
    "'''\n",
    "def entropies_of_df_entries(df, col, model_prefix):\n",
    "    entropies = []\n",
    "    for data in df[col]:\n",
    "        token_counts= encode_text(data, model_prefix)\n",
    "        entropies.append(dit_shannon(token_counts))\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "#Entropies of df test\n",
    "enty = entropies_of_df_entries(df,'contents','output_sp_bpe_modal')\n",
    "\n",
    "entropy1 = dit_shannon(Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, \n",
    "                                '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1}))\n",
    "\n",
    "entropy2 = dit_shannon(Counter({'▁is': 2, '▁the': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1}))\n",
    "\n",
    "assert(enty == [entropy1, entropy2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish this such that is finds the entropy of the entire corpus \\\n",
    "#       and preserves the individual token frequencies so that we can   \\\n",
    "#      compute the most common tokens\n",
    "\n",
    "# def entropy_of_whole_corpus(df, col, model_prefix):\n",
    "#     '''Returns a dictionary of the entropies of each token in a dataframe corpus'''\n",
    "#     entropies = {}\n",
    "#     token_counts = encode_text(pd.concat[col], model_prefix)\n",
    "#     entropies.append(dit_shannon(token_counts))\n",
    "#     return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG DELETE\n",
    "# TODO: Do we need this function?\n",
    "import math\n",
    "def manual_shannon(token_freqs):\n",
    "    sum = 0\n",
    "    for i in token_freqs:\n",
    "        sum += i * math.log(1/i, 2)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG DELETE\n",
    "# TODO: Do we need this function?\n",
    "def sort_token_data(token_data):\n",
    "    return sorted(token_data.items() ,  key=lambda x: x[1][\"Occurrences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBest Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hide\n",
    "# Create a dataframe of the requirements, source code and test case data for LIBest\n",
    "# Create a sentencepiece model using the entire LIBest corpus\n",
    "LIB_corpus_df = simulate_getting_dataframes_from_mongo()\n",
    "LIB_model = gen_sp_model(LIB_corpus_df, output='LIBest', model_name='_sp_bpe_modal', cols=['contents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
