{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp desc.metrics.main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of your data\n",
    "\n",
    "> This module comprises some of the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
    ">\n",
    "> - Descriptive statistics\n",
    "> - Software Metrics\n",
    "> - Information Theory\n",
    "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
    "> - Inference: Probabilistic and Causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifically in this module\n",
    "\n",
    "> - Cyclomatic complexity (CYCLO)\n",
    "> - Number of lines of code (NLOC)\n",
    "> - Lack of Cohesion of Methods 5 (LCOM5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Tools\n",
    "\n",
    "> Languages this module plans to be able to process: Java, Javascript, Python, C++, C, Scala\n",
    "##### Current Limitations\n",
    "> - CYCLO/NLOC\n",
    "> > - Can measure: Java\n",
    "> > - Cannot measure: Python\n",
    "> > - Can possibly measure: C, C++, Scala, Javascript\n",
    "> - LCOM5\n",
    "> > - Can measure: Java\n",
    "> > - Cannot measure: Python, C++, Scala, Javascript\n",
    "> > - Undefined: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lizard in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (1.17.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tree_sitter in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (0.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from pandas) (1.18.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scipy in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from scipy) (1.18.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from matplotlib) (1.18.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: chardet in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: bs4 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from bs4) (4.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/wilsmccreight/Desktop/Research/ds4se/notebook-venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install metrics # Outdated Cyclomatic Complexity tool\n",
    "!pip install lizard\n",
    "!pip install tree_sitter\n",
    "\n",
    "# The below modules need to be loaded when not being used in Google Colab\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install chardet\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from numpy import mean, std\n",
    "from statistics import median\n",
    "from scipy.stats import sem, t\n",
    "import lizard\n",
    "import matplotlib.pyplot as plt\n",
    "from tree_sitter import Language, Parser, Node\n",
    "#Decoding files\n",
    "import chardet\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "\n",
    "# TODO: Remove when mongo call is implemented\n",
    "import os\n",
    "\n",
    "\n",
    "from desc_metrics_parser_java import ParserJava\n",
    "from desc_metrics_parser_cpp import ParserCPP\n",
    "from desc_metrics_parser_python import ParserPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'tree-sitter-java' already exists and is not an empty directory.\n",
      "Cloning into 'tree-sitter-cpp'...\n",
      "remote: Enumerating objects: 83, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
      "remote: Total 1327 (delta 44), reused 59 (delta 31), pack-reused 1244\u001b[K\n",
      "Receiving objects: 100% (1327/1327), 38.36 MiB | 12.67 MiB/s, done.\n",
      "Resolving deltas: 100% (862/862), done.\n",
      "Cloning into 'tree-sitter-python'...\n",
      "remote: Enumerating objects: 84, done.\u001b[K\n",
      "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
      "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
      "remote: Total 2135 (delta 40), reused 58 (delta 25), pack-reused 2051\u001b[K\n",
      "Receiving objects: 100% (2135/2135), 17.91 MiB | 15.47 MiB/s, done.\n",
      "Resolving deltas: 100% (1358/1358), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tree-sitter/tree-sitter-java\n",
    "!git clone https://github.com/tree-sitter/tree-sitter-cpp\n",
    "!git clone https://github.com/tree-sitter/tree-sitter-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_unicode(file_path):\n",
    "    \"\"\"Detects file encoding and returns unicode. Inspired by http://reinvantveer.github.io/2017/05/19/unicode-dammit.html\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        detection = chardet.detect(f.read())\n",
    "        \n",
    "    enc = detection[\"encoding\"]\n",
    "    if detection[\"encoding\"] == \"ascii\":\n",
    "        with open(file_path, encoding=\"ascii\") as f:\n",
    "            data = f.read()\n",
    "    elif detection[\"encoding\"] == \"ISO-8859-9\":\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            enc = \"utf-8\"\n",
    "            data = f.read()\n",
    "    else:\n",
    "        try:\n",
    "            # Try to open as non unicode file\n",
    "            with open(file_path, encoding=detection[\"encoding\"]) as f:\n",
    "                data = f.read()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Cannot return dictionary from empty or invalid csv file {file_path} due to {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(f\"Cannot return dictionary from empty or invalid csv file {file_path}\")\n",
    "\n",
    "    return UnicodeDammit(data).unicode_markup, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_getting_dataframes_from_mongo(folder_path):\n",
    "    \"\"\"Loads files from a specified folder into a pandas dataframe\"\"\"\n",
    "    corpus_data = {\"system\": [], \"name\": [], \"ground_truth\": [], \"contents\": [], \"encoding\": []}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not os.path.isdir(os.path.join(folder_path, file)) and file != \".DS_Store\":\n",
    "            corpus_data[\"system\"].append(None)\n",
    "            corpus_data[\"name\"].append(file)\n",
    "            corpus_data[\"ground_truth\"].append(\"src\")\n",
    "            contents, enc = get_unicode(os.path.join(folder_path, file))\n",
    "            corpus_data['encoding'].append(enc)\n",
    "            corpus_data['contents'].append(contents)\n",
    "    corpus_df = pd.DataFrame(data = corpus_data)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def add_mccabe_metrics(df, data_col, name_col):\n",
    "    \"\"\"Adds information about function length and cyclomatic complexity for classes to a dataframe\"\"\"\n",
    "    num_funcs = []\n",
    "    class_ccn = []\n",
    "    avg_func_ccn = []\n",
    "    avg_func_nloc = []\n",
    "    for i in range(len(df)):\n",
    "        file_num_funcs = []\n",
    "        file_class_ccn = []\n",
    "        file_avg_func_ccn = []\n",
    "        file_avg_func_nloc = []\n",
    "        metrics = lizard.analyze_file.analyze_source_code(df[name_col][i], df[data_col][i])\n",
    "        class_dict = {}\n",
    "        for func in metrics.function_list:\n",
    "            class_name = '::'.join(func.name.split(\"::\")[:-1])\n",
    "            if class_name in class_dict:\n",
    "                class_dict[class_name].append(func)\n",
    "            else:\n",
    "                class_dict[class_name] = [func]\n",
    "        for class_key in class_dict:\n",
    "            total_class_ccn = 0\n",
    "            total_class_nloc = 0\n",
    "            for func in class_dict[class_key]:\n",
    "                total_class_ccn += func.cyclomatic_complexity\n",
    "                total_class_nloc += func.length\n",
    "            file_num_funcs.append(len(class_dict[class_key]))\n",
    "            file_class_ccn.append(total_class_ccn)\n",
    "            file_avg_func_ccn.append(total_class_ccn/len(class_dict[class_key]))\n",
    "            file_avg_func_nloc.append(total_class_nloc/len(class_dict[class_key]))\n",
    "\n",
    "        num_funcs.append(file_num_funcs)\n",
    "        class_ccn.append(file_class_ccn)\n",
    "        avg_func_ccn.append(file_avg_func_ccn)\n",
    "        avg_func_nloc.append(file_avg_func_nloc)\n",
    "\n",
    "    df[\"num_funcs\"] = num_funcs\n",
    "    df[\"class_ccn\"] = class_ccn\n",
    "    df[\"avg_func_ccn\"] = avg_func_ccn\n",
    "    df[\"avg_func_nloc\"] = avg_func_nloc\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser_builds(path=None):\n",
    "    \"\"\"Creates a dictionary of tree-sitter parsers for select languages\"\"\"\n",
    "    Language.build_library(\n",
    "        # Store the library in the `build` directory\n",
    "        'build/my-languages.so',\n",
    "\n",
    "        # Include one or more languages\n",
    "        [\n",
    "            'tree-sitter-java',\n",
    "            'tree-sitter-cpp',\n",
    "            'tree-sitter-python'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    JAVA_LANGUAGE = Language('build/my-languages.so', 'java')\n",
    "    CPP_LANGUAGE = Language('build/my-languages.so', 'cpp')\n",
    "    PY_LANGUAGE = Language('build/my-languages.so', 'python')\n",
    "    \n",
    "    return {\"java\":JAVA_LANGUAGE, \"cpp\":CPP_LANGUAGE, \"py\":PY_LANGUAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcom5_parser(ext):\n",
    "    if ext == \"java\":\n",
    "        return ParserJava()\n",
    "    if ext == \"cpp\":\n",
    "        return ParserCPP()\n",
    "    if ext == \"py\":\n",
    "        return ParserPython()\n",
    "    print(f\"ERROR: No LCOM5 parser available for .{ext} extension files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_lcom5(tree, extension, file_bytes, name):\n",
    "    \"\"\"Parses the syntax tree of code to calculate the LCOM5 of its classes\"\"\"\n",
    "    parser = lcom5_parser(extension)\n",
    "    if not parser:\n",
    "        return [\"Undefined\"]\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    class_nodes = parser.find_class_nodes(root_node)\n",
    "    class_method_names = []\n",
    "    class_field_names = []\n",
    "    class_dfc = [] # Distinct field calls, as per the definition of LCOM5\n",
    "    for node in enumerate(class_nodes):\n",
    "        class_method_names.append(parser.find_method_names(node[1], file_bytes))\n",
    "        class_field_names.append(parser.find_field_names(node[1], class_method_names[node[0]], file_bytes))\n",
    "        class_dfc.append(parser.distinct_field_calls(node[1], class_field_names[node[0]], file_bytes))\n",
    "    lcom5_list = []\n",
    "    for j in range(len(class_nodes)):\n",
    "        num_fields = len(class_field_names[j])\n",
    "        num_meths = len(class_method_names[j])\n",
    "        num_dac = class_dfc[j]\n",
    "        numerator = num_dac - (num_meths*num_fields)\n",
    "        denominator = num_fields - (num_meths*num_fields)\n",
    "        if denominator == 0:\n",
    "            lcom5_list.append(\"Undefined\")\n",
    "        else:\n",
    "            lcom5_list.append(numerator/denominator)\n",
    "    return lcom5_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_lcom5(df, col):\n",
    "    \"\"\"Adds a column with the LCOM5 of each class of each file to a dataframe\"\"\"\n",
    "    lang_builds = create_parser_builds()\n",
    "    parser = Parser()\n",
    "    class_lcom5 = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        ext = df[\"name\"][i].split('.')[-1]\n",
    "        parser.set_language(lang_builds[ext])\n",
    "        enc = df[\"encoding\"][i]\n",
    "        tree = parser.parse(bytes(df[\"contents\"][i], df[\"encoding\"][i]))\n",
    "        class_lcom5.append(calculate_lcom5(tree, ext, bytes(df[\"contents\"][i], df[\"encoding\"][i]), df[\"name\"][i]))\n",
    "    df[\"class_lcom5\"] = class_lcom5\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def flatten_lol(list_list):\n",
    "    \"\"\"Takes in a list of lists and flattens it, returning a list of each entry\"\"\"\n",
    "    flattened_list = []\n",
    "    for sublist in list_list:\n",
    "        for entry in sublist:\n",
    "            flattened_list.append(entry)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def display_numeric_col_stats(col, conf = 0.95, sig_figs = 4, clean=True, verbose_clean=False):\n",
    "    \"\"\"Computes statistical metrics about the entries in a dataframe column or list\"\"\"\n",
    "    previous_length = len(col)\n",
    "    numeric_types = [int, float, complex]\n",
    "    if clean: col = [x for x in col if type(x) in numeric_types]\n",
    "    if verbose_clean: print(f\"Cleaning removed {previous_length - len(col)} non-numeric entries\")\n",
    "\n",
    "    if len(col) < 1:\n",
    "        print(\"Error, data must contain at least one valid entry to display statistics\")\n",
    "        return\n",
    "\n",
    "    print(\"Min =\", round(min(col), sig_figs))\n",
    "    print(\"Max =\", round(max(col), sig_figs))\n",
    "    print(\"Average =\", round(mean(col), sig_figs))\n",
    "    print(\"Median =\", round(median(col), sig_figs))\n",
    "    print(\"Standard Deviation =\", round(std(col), sig_figs))\n",
    "    \n",
    "    n = len(col)\n",
    "    m = mean(col)\n",
    "    std_err = sem(col)\n",
    "    h = std_err * t.ppf((1 + conf) / 2, n - 1)\n",
    "\n",
    "    start = m - h\n",
    "    end = m + h\n",
    "    print(f\"{conf} of data points fall between {round(start, sig_figs)} and {round(end, sig_figs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def display_numeric_col_hist(col, col_name=\"Metric\", num_bins=20, clean=True, verbose_clean=False):\n",
    "    \"\"\"Displays a histogram with a customized number of bins for the data in a specified dataframe column or list\"\"\"\n",
    "    previous_length = len(col)\n",
    "    numeric_types = [int, float, complex]\n",
    "    if clean: col = [x for x in col if type(x) in numeric_types]\n",
    "    if verbose_clean: print(f\"Cleaning removed {previous_length - len(col)} non-numeric entries\")\n",
    "\n",
    "    if len(col) < 1:\n",
    "        print(\"Error, data must contain at least one valid entry to display histogram\")\n",
    "        return    \n",
    "\n",
    "    rng = max(col) - min(col)\n",
    "    num = len(col)\n",
    "    stnd_dev = std(col)\n",
    "\n",
    "    plt.hist(col, num_bins, color=\"blue\", alpha=0.5, edgecolor=\"black\", linewidth=1.0)\n",
    "    plt.title(col_name + \" Histogram\")\n",
    "    plt.ylabel(\"Value  Range  Occurrences\")\n",
    "    plt.xlabel(col_name)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
