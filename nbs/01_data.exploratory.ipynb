{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "01_data.exploratory.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U126SlRFD_uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default_exp data.exploratory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbi0boL5D_uQ",
        "colab_type": "text"
      },
      "source": [
        "# Exploration of your data\n",
        "\n",
        "> This module comprises all the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
        ">\n",
        "> - Descriptive statistics\n",
        "> - Software Metrics\n",
        "> - Information Theory\n",
        "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
        "> - Inference: Probabilistic and Causal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6q5FioqIt8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "ff8d4a6b-91e9-4a56-dd2a-b50af038cbd8"
      },
      "source": [
        "!pip install dit\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dit in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from dit) (1.17.5)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from dit) (1.12.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from dit) (0.5.5)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (from dit) (0.7.2)\n",
            "Requirement already satisfied: scipy>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from dit) (1.4.1)\n",
            "Requirement already satisfied: debtcollector in /usr/local/lib/python3.6/dist-packages (from dit) (2.0.0)\n",
            "Requirement already satisfied: boltons in /usr/local/lib/python3.6/dist-packages (from dit) (20.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from dit) (2.4)\n",
            "Requirement already satisfied: wrapt>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from debtcollector->dit) (1.11.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from debtcollector->dit) (5.4.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->dit) (4.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-sHbVxx1_-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import sentencepiece as sp\n",
        "import dit\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EirQw8HqD_uT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #hide\n",
        "# from nbdev.showdoc import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TFOIobLD_ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import dataframe from MongoDB\n",
        "def simulate_getting_dataframes_from_mongo():\n",
        "  requirements = {'file_name': [], 'contents': []}\n",
        "  path = \"./requirements\"\n",
        "  for file in os.listdir(path):\n",
        "    requirements['file_name'].append(file)\n",
        "    with open (os.path.join(path, file), \"r\") as f:\n",
        "      requirements['contents'].append(f.read())\n",
        "  source_code = {'file_name': [], 'contents': []}\n",
        "  path = \"./source_code\"\n",
        "  for file in os.listdir(\"./source_code\"):\n",
        "    source_code['file_name'].append(file)\n",
        "    with open (os.path.join(path, file), \"r\") as f:\n",
        "      source_code['contents'].append(f.read())\n",
        "  req_df = pd.DataFrame(data = requirements)\n",
        "  src_df = pd.DataFrame(data = source_code)\n",
        "  return req_df, src_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gy9Mn-h2oPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def df_to_txt_file(df, output, cols):\n",
        "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
        "    if cols is None: cols = list(df.columns)\n",
        "    merged_df = pd.concat([df[col] for col in cols])\n",
        "    \n",
        "    with open(output + '_text.txt', 'w') as f:\n",
        "        f.write('\\n'.join(list(merged_df)))\n",
        "    return output + '_text.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGfft5S6IldP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def gen_sp_model(df, output, model_name, cols = None):\n",
        "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
        "    fname = df_to_txt_file(df, output, cols)\n",
        "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output + model_name} --hard_vocab_limit=false --model_type=bpe')\n",
        "    return output + model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_NEGx9_cHsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def encode_and_count(df, model_prefix):\n",
        "    sp_processor = sp.SentencePieceProcessor()\n",
        "    sp_processor.Load(f\"{model_prefix}.model\")\n",
        "    tok_freq = {}\n",
        "    num_tokens = 0\n",
        "    for file in df.contents:\n",
        "        encoding = sp_processor.encode_as_pieces(file)\n",
        "        for piece in encoding:\n",
        "            tok_freq.setdefault(piece, {\"Occurrences\": 0})\n",
        "            tok_freq[piece][\"Occurrences\"] += 1\n",
        "            num_tokens += 1\n",
        "    return tok_freq, num_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZPnewVLeUOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def add_frequency_and_id(tok_freq, num_tokens):\n",
        "    counter = 0\n",
        "    for token in tok_freq:\n",
        "        counter += 1\n",
        "        tok_freq[token][\"Frequency\"] = tok_freq[token][\"Occurrences\"]/num_tokens\n",
        "        tok_freq[token][\"Outcome_Symbol\"] = '0' * (10-len(str(counter))) + str(counter)\n",
        "    return tok_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9KbUN3qhDPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "import math\n",
        "def manual_shannon(token_freqs):\n",
        "    sum = 0\n",
        "    for i in token_freqs:\n",
        "        sum += i * math.log(1/i, 2)\n",
        "    return sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX-k-E1OjOGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def entropy_of_corpus(token_data):\n",
        "    tokens = []\n",
        "    frequencies = []\n",
        "    for i in token_data:\n",
        "        tokens.append(token_data[i][\"Outcome_Symbol\"])\n",
        "        frequencies.append(token_data[i][\"Frequency\"])\n",
        "    d = dit.Distribution(tokens, frequencies)\n",
        "    return dit.shannon.entropy(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT_RC1iMjWH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def entropy_of_file(token_data, file_text, model_prefix):\n",
        "    sp_processor = sp.SentencePieceProcessor()\n",
        "    sp_processor.Load(f\"{model_prefix}.model\")\n",
        "    encoding = sp_processor.encode_as_pieces(file_text)\n",
        "    tokens = []\n",
        "    for piece in encoding:\n",
        "        tokens.append(piece)\n",
        "    tokens = set(tokens)\n",
        "    frequencies = [token_data[tok][\"Frequency\"] for tok in tokens ]\n",
        "    return manual_shannon(frequencies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5feE1QZjj2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export \n",
        "def entropy_of_all_files(df, token_data, model_prefix):\n",
        "    entropies = []\n",
        "    for file in df.contents:\n",
        "        entropies.append(entropy_of_file(token_data, file, model_prefix))\n",
        "    return entropies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMrQhFX8mpvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def sort_token_data(token_data):\n",
        "    return sorted(token_data.items() ,  key=lambda x: x[1][\"Occurrences\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJxMpyFn2oUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compute entropy of all the files per system and calculate mean, std, median, and std for median absolute deviation. The idea is to create confidence intervals for each system/dataset\n",
        "req_df, src_df = simulate_getting_dataframes_from_mongo()\n",
        "\n",
        "model_prefix = gen_sp_model(req_df, output='requirements', model_name='_sp_bpe_modal', cols=['contents'])\n",
        "tok_freq, num_tokens = encode_and_count(req_df, model_prefix)\n",
        "token_data = add_frequency_and_id(tok_freq, num_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSlHID6LnBe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in token_data:\n",
        "#   print(token_data[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhzf7GItTkEO",
        "colab_type": "code",
        "outputId": "82504185-f7d3-4bed-d70f-5cc2b83f7b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(entropy_of_corpus(token_data))\n",
        "entropies = entropy_of_all_files(req_df, token_data, model_prefix)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.518520436410853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AidMsM_SoiDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b81fe3c6-5c08-4252-a9f5-6852a58a1f08"
      },
      "source": [
        "from scipy.stats import sem, t\n",
        "from numpy import mean\n",
        "import statistics as stat\n",
        "print(\"Max entropy:\", max(entropies))\n",
        "print(\"Min entropy:\", min(entropies))\n",
        "print(\"Average entropy:\", mean(entropies))\n",
        "print(\"Median entropy:\", stat.median(entropies))\n",
        "\n",
        "confidence = 0.95\n",
        "n = len(entropies)\n",
        "m = mean(entropies)\n",
        "std_err = sem(entropies)\n",
        "h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
        "\n",
        "start = m - h\n",
        "end = m + h\n",
        "print(f\"95% of entropies fall within {start} and {end}\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max entropy: 4.081000217077757\n",
            "Min entropy: 0.847693014640494\n",
            "Average entropy: 2.421914873609174\n",
            "Median entropy: 2.355904742754068\n",
            "95% of entropies fall within 2.204935576452618 and 2.63889417076573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epTo4OCvoiQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUYLZvQqqjv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9OCCktriZoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a186b59-ef0d-4acb-c356-75cf0d901122"
      },
      "source": [
        "freq = [.8, .1, .1]\n",
        "toks = [str(i) for i in range(len(freq))]\n",
        "\n",
        "d = dit.Distribution(toks, freq)\n",
        "print(dit.shannon.entropy(d))\n",
        "print(manual_shannon(freq))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9219280948873623\n",
            "0.9219280948873625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFycFx-ihkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXSfVlVo2oYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rank the system/datasets according to the confidence intervals\n",
        "#Compute the confidence intervals for all cross-entropy values\n",
        "#Rank the systems/datasets according to cross-entropy values\n",
        "#Top 50 most frequent tokens of each system and corpus (one system has generally two corpora)\n",
        "#Top 50 least frequent tokes of each system and corpus\n",
        "#What are the tokens that are in the target and not in the source (and the other way around)? Compute the distribution for those tokens\n",
        "#What are the mutual tokens (source and target)? please compute distribution\n",
        "#-Compute confidence intervals for the software metrics on source code (e.g., cyclo, loc, lcom5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU3QLYSd2RAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Visualize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFr71USN2Sod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Push updated fields to Mongo"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}