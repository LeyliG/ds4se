{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "01_data.exploratory.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U126SlRFD_uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default_exp data.exploratory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbi0boL5D_uQ",
        "colab_type": "text"
      },
      "source": [
        "# Exploration of your data\n",
        "\n",
        "> This module comprises all the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
        ">\n",
        "> - Descriptive statistics\n",
        "> - Software Metrics\n",
        "> - Information Theory\n",
        "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
        "> - Inference: Probabilistic and Causal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6q5FioqIt8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install dit\n",
        "!pip install sencencepiece"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-sHbVxx1_-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import sentencepiece as sp\n",
        "import dit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EirQw8HqD_uT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #hide\n",
        "# from nbdev.showdoc import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TFOIobLD_ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import dataframe from MongoDB\n",
        "import os\n",
        "def simulate_getting_dataframes_from_mongo():\n",
        "  requirements = {'file_name': [], 'contents': []}\n",
        "  path = \"./requirements\"\n",
        "  for file in os.listdir(path):\n",
        "    requirements['file_name'].append(file)\n",
        "    with open (os.path.join(path, file), \"r\") as f:\n",
        "      requirements['contents'].append(f.read())\n",
        "  source_code = {'file_name': [], 'contents': []}\n",
        "  path = \"./source_code\"\n",
        "  for file in os.listdir(\"./source_code\"):\n",
        "    source_code['file_name'].append(file)\n",
        "    with open (os.path.join(path, file), \"r\") as f:\n",
        "      source_code['contents'].append(f.read())\n",
        "  req_df = pd.DataFrame(data = requirements)\n",
        "  src_df = pd.DataFrame(data = source_code)\n",
        "  return req_df, src_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gy9Mn-h2oPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def df_to_txt_file(df, output, cols):\n",
        "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
        "    if cols is None: cols = list(df.columns)\n",
        "    merged_df = pd.concat([df[col] for col in cols])\n",
        "    \n",
        "    with open(output + 'text.txt', 'w') as f:\n",
        "        f.write('\\n'.join(list(merged_df)))\n",
        "    return output + 'text.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGfft5S6IldP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def gen_sp_model(df, output, model_name, cols = None):\n",
        "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
        "    fname = df_to_txt_file(df, output, cols)\n",
        "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output + model_name} --hard_vocab_limit=false --model_type=bpe')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJxMpyFn2oUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compute entropy of all the files per system and calculate mean, std, median, and std for median absolute deviation. The idea is to create confidence intervals for each system/dataset\n",
        "req_df, src_df = get_dataframes_from_mongo()\n",
        "gen_sp_model(req_df, output='requirements', model_name='sp_bpe_modal', cols=['contents'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqfrsNY3FGbo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d0f720a-dceb-42cb-ae4b-7a2b51be3205"
      },
      "source": [
        "d = dit.Distribution(['1', '2', '3', '4'], [0.125, 0.125, 0.25, 0.5])\n",
        "print(dit.shannon.entropy(d))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXSfVlVo2oYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rank the system/datasets according to the confidence intervals\n",
        "#Compute the confidence intervals for all cross-entropy values\n",
        "#Rank the systems/datasets according to cross-entropy values\n",
        "#Top 50 most frequent tokens of each system and corpus (one system has generally two corpora)\n",
        "#Top 50 least frequent tokes of each system and corpus\n",
        "#What are the tokens that are in the target and not in the source (and the other way around)? Compute the distribution for those tokens\n",
        "#What are the mutual tokens (source and target)? please compute distribution\n",
        "#-Compute confidence intervals for the software metrics on source code (e.g., cyclo, loc, lcom5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU3QLYSd2RAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Visualize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFr71USN2Sod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Push updated fields to Mongo"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}