{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "01_data.exploratory.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U126SlRFD_uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default_exp data.exploratory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbi0boL5D_uQ",
        "colab_type": "text"
      },
      "source": [
        "# Exploration of your data\n",
        "\n",
        "> This module comprises all the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
        ">\n",
        "> - Descriptive statistics\n",
        "> - Software Metrics\n",
        "> - Information Theory\n",
        "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
        "> - Inference: Probabilistic and Causal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6q5FioqIt8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "80f2d970-2c32-49f5-f2cb-5a3fb68251e4"
      },
      "source": [
        "!pip install dit\n",
        "!pip install sentencepiece\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dit in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: scipy>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from dit) (1.4.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from dit) (1.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from dit) (2.4)\n",
            "Requirement already satisfied: debtcollector in /usr/local/lib/python3.6/dist-packages (from dit) (2.0.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (from dit) (0.7.2)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from dit) (0.5.5)\n",
            "Requirement already satisfied: boltons in /usr/local/lib/python3.6/dist-packages (from dit) (20.0.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from dit) (1.17.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->dit) (4.4.1)\n",
            "Requirement already satisfied: wrapt>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from debtcollector->dit) (1.11.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from debtcollector->dit) (5.4.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-sHbVxx1_-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import sentencepiece as sp\n",
        "import dit\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EirQw8HqD_uT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #hide\n",
        "# from nbdev.showdoc import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TFOIobLD_ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import dataframe from MongoDB\n",
        "def simulate_getting_dataframes_from_mongo():\n",
        "  requirements = {'file_name': [], 'contents': []}\n",
        "  path = \"./requirements\"\n",
        "  for file in os.listdir(path):\n",
        "    requirements['file_name'].append(file)\n",
        "    with open (os.path.join(path, file), \"r\") as f:\n",
        "      requirements['contents'].append(f.read())\n",
        "  source_code = {'file_name': [], 'contents': []}\n",
        "  path = \"./source_code\"\n",
        "  for file in os.listdir(\"./source_code\"):\n",
        "    source_code['file_name'].append(file)\n",
        "    with open (os.path.join(path, file), \"r\") as f:\n",
        "      source_code['contents'].append(f.read())\n",
        "  req_df = pd.DataFrame(data = requirements)\n",
        "  src_df = pd.DataFrame(data = source_code)\n",
        "  return req_df, src_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gy9Mn-h2oPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def df_to_txt_file(df, output, cols):\n",
        "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
        "    if cols is None: cols = list(df.columns)\n",
        "    merged_df = pd.concat([df[col] for col in cols])\n",
        "    \n",
        "    with open(output + '_text.txt', 'w') as f:\n",
        "        f.write('\\n'.join(list(merged_df)))\n",
        "    return output + '_text.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGfft5S6IldP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def gen_sp_model(df, output, model_name, cols = None):\n",
        "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
        "    fname = df_to_txt_file(df, output, cols)\n",
        "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output + model_name} --hard_vocab_limit=false --model_type=bpe')\n",
        "    return output + model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_NEGx9_cHsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def encode_and_count(df, model_prefix):\n",
        "    sp_processor = sp.SentencePieceProcessor()\n",
        "    sp_processor.Load(f\"{model_prefix}.model\")\n",
        "    tok_freq = {}\n",
        "    num_tokens = 0\n",
        "    for file in df.contents:\n",
        "        encoding = sp_processor.encode_as_pieces(file)\n",
        "        for piece in encoding:\n",
        "            tok_freq.setdefault(piece, {\"Occurrences\": 0})\n",
        "            tok_freq[piece][\"Occurrences\"] += 1\n",
        "            num_tokens += 1\n",
        "    return tok_freq, num_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZPnewVLeUOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def add_frequency_and_id(tok_freq, num_tokens):\n",
        "    counter = 0\n",
        "    for token in tok_freq:\n",
        "        counter += 1\n",
        "        tok_freq[token][\"Frequency\"] = tok_freq[token][\"Occurrences\"]/num_tokens\n",
        "        tok_freq[token][\"Outcome_Symbol\"] = '0' * (10-len(str(counter))) + str(counter)\n",
        "    return tok_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX-k-E1OjOGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export\n",
        "def compute_entropy(token_data):\n",
        "    tokens = []\n",
        "    frequencies = []\n",
        "    for i in token_data:\n",
        "        tokens.append(i[1][\"Outcome_Symbol\"])\n",
        "        frequencies.append(i[1][\"Frequency\"])\n",
        "    d = dit.Distribution(tokens, frequencies)\n",
        "    return dit.shannon.entropy(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJxMpyFn2oUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compute entropy of all the files per system and calculate mean, std, median, and std for median absolute deviation. The idea is to create confidence intervals for each system/dataset\n",
        "req_df, src_df = simulate_getting_dataframes_from_mongo()\n",
        "\n",
        "model_prefix = gen_sp_model(req_df, output='requirements', model_name='_sp_bpe_modal', cols=['contents'])\n",
        "tok_freq, num_tokens = encode_and_count(req_df, model_prefix)\n",
        "token_data = add_frequency_and_id(tok_freq, num_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhzf7GItTkEO",
        "colab_type": "code",
        "outputId": "7193f4d2-01a3-4b0b-eb37-1f384f6849d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sorted_freq = sorted(token_data.items() ,  key=lambda x: x[1][\"Occurrences\"])\n",
        "print(compute_entropy(sorted_freq))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.518520436410853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXSfVlVo2oYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rank the system/datasets according to the confidence intervals\n",
        "#Compute the confidence intervals for all cross-entropy values\n",
        "#Rank the systems/datasets according to cross-entropy values\n",
        "#Top 50 most frequent tokens of each system and corpus (one system has generally two corpora)\n",
        "#Top 50 least frequent tokes of each system and corpus\n",
        "#What are the tokens that are in the target and not in the source (and the other way around)? Compute the distribution for those tokens\n",
        "#What are the mutual tokens (source and target)? please compute distribution\n",
        "#-Compute confidence intervals for the software metrics on source code (e.g., cyclo, loc, lcom5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU3QLYSd2RAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Visualize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFr71USN2Sod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Push updated fields to Mongo"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}