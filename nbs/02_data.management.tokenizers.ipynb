{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.management.tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management Tokenizers Core\n",
    "\n",
    "> This module contains all of the tokenizers supported by `ds4se`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import ds4se\n",
    "\n",
    "import sentencepiece as sp\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pandas_to_txt_file(df: pd.DataFrame, output_path: Path, cols) -> Path:\n",
    "    if cols is None:\n",
    "        cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "\n",
    "    with open(str(output_path / \"ds4se_data.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(list(merged_df)))\n",
    "\n",
    "    return output_path / \"ds4se_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DS4SETokenizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize(self, example: str) -> List[str]:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(path: Path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SentencePieceTokenizer(DS4SETokenizer):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(self, tokenizer)\n",
    "\n",
    "    def tokenize(self, example: str) -> List[str]:\n",
    "        return self.tokenizer.EncodeAsPieces(example)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pandas(\n",
    "        df: pd.DataFrame, output_path: Path, cols: Optional[List[str]] = None\n",
    "    ) -> SentencePieceTokenizer:\n",
    "        f_path = pandas_to_txt_file(df, cols, output_path)\n",
    "        sp.SentencePieceTrainer.train(\n",
    "            f\"--input={f_path} --model_prefix={output_path}/sentencepiece --hard_vocab_limit=false\"\n",
    "        )\n",
    "\n",
    "        return SentencePieceTokenizer.from_pretrained(\n",
    "            output_path / \"sentencepiece.model\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(f_path: Path) -> SentencePieceTokenizer:\n",
    "        tokenizer = sp.SentencePieceProcessor()\n",
    "        tokenizer.Load(str(f_path))\n",
    "\n",
    "        return SentencePieceTokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ByteLevelTokenizer(DS4SETokenizer):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(self, tokenizer)\n",
    "\n",
    "    def tokenize(self, example: str) -> List[str]:\n",
    "        return self.tokenizer.EncodeAsPieces(example)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pandas(\n",
    "        df: pd.DataFrame, output_path: Path, cols: Optional[List[str]] = None\n",
    "    ) -> SentencePieceTokenizer:\n",
    "        f_path = pandas_to_txt_file(df, cols, output_path)\n",
    "        sp.SentencePieceTrainer.train(\n",
    "            f\"--input={f_path} --model_prefix={output_path}/sentencepiece --hard_vocab_limit=false\"\n",
    "        )\n",
    "\n",
    "        return SentencePieceTokenizer.from_pretrained(\n",
    "            output_path / \"sentencepiece.model\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(f_path: Path) -> SentencePieceTokenizer:\n",
    "        tokenizer = sp.SentencePieceProcessor()\n",
    "        tokenizer.Load(str(f_path))\n",
    "\n",
    "        return SentencePieceTokenizer(tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
