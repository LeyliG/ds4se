# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_data.management.tokenizers.ipynb (unless otherwise specified).

__all__ = ['pandas_to_txt_file', 'DS4SETokenizer', 'SentencePieceTokenizer', 'ByteLevelTokenizer']

# Cell
import pandas as pd
import sentencepiece as sp

from pathlib import Path
from typing import List, Optional

# Cell
def pandas_to_txt_file(df: pd.DataFrame, output_path: Path, cols) -> Path:
    if cols is None:
        cols = list(df.columns)
    merged_df = pd.concat([df[col] for col in cols])

    with open(str(output_path / "ds4se_data.txt"), "w") as f:
        f.write("\n".join(list(merged_df)))

    return output_path / "ds4se_data.txt"

# Cell
class DS4SETokenizer:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def tokenize(self, example: str) -> List[str]:
        pass

    @staticmethod
    def from_pretrained(path: Path):
        pass

# Cell
class SentencePieceTokenizer(DS4SETokenizer):
    def __init__(self, tokenizer):
        super().__init__(self, tokenizer)

    def tokenize(self, example: str) -> List[str]:
        return self.tokenizer.EncodeAsPieces(example)

    @staticmethod
    def from_pandas(
        df: pd.DataFrame, output_path: Path, cols: Optional[List[str]] = None
    ) -> DS4SETokenizer:
        f_path = pandas_to_txt_file(df, cols, output_path)
        sp.SentencePieceTrainer.train(
            f"--input={f_path} --model_prefix={output_path}/sentencepiece --hard_vocab_limit=false"
        )

        return SentencePieceTokenizer.from_pretrained(
            output_path / "sentencepiece.model"
        )

    @staticmethod
    def from_pretrained(f_path: Path) -> DS4SETokenizer:
        tokenizer = sp.SentencePieceProcessor()
        tokenizer.Load(str(f_path))

        return SentencePieceTokenizer(tokenizer)

# Cell
class ByteLevelTokenizer(DS4SETokenizer):
    def __init__(self, tokenizer):
        super().__init__(self, tokenizer)

    def tokenize(self, example: str) -> List[str]:
        return self.tokenizer.EncodeAsPieces(example)

    @staticmethod
    def from_pandas(
        df: pd.DataFrame, output_path: Path, cols: Optional[List[str]] = None
    ) -> DS4SETokenizer:
        f_path = pandas_to_txt_file(df, cols, output_path)
        sp.SentencePieceTrainer.train(
            f"--input={f_path} --model_prefix={output_path}/sentencepiece --hard_vocab_limit=false"
        )

        return SentencePieceTokenizer.from_pretrained(
            output_path / "sentencepiece.model"
        )

    @staticmethod
    def from_pretrained(f_path: Path) -> DS4SETokenizer:
        tokenizer = sp.SentencePieceProcessor()
        tokenizer.Load(str(f_path))

        return SentencePieceTokenizer(tokenizer)