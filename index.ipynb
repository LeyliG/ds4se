{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import ds4se.facade as facade\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ds4se in c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se (0.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "pip install ds4se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ds4se\n",
    "\n",
    "> Data Science for Software Engieering (ds4se) is an academic initiative to perform exploratory analysis on software engineering artifacts and metadata. Data Management, Analysis, and Benchmarking for DL and Traceability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This documentation composed of 4 parts: \n",
    "\n",
    "1)architecture\n",
    "\n",
    "2)deployment\n",
    "\n",
    "3)installation \n",
    "\n",
    "4)usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the Architecture diagram of the DS4Se library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DS4SE.png](DS4SE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users of the DS4SE API will pass in either strings or pandas dataframes that consist of content of either source or target artifacts as input, and get different analytical results of the input depending on the function user called.  \n",
    "\n",
    "The DS4SE library is divided into two parts: Traceability and Analysis, corresponind to different usages of the API. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traceability\n",
    "\n",
    "The traceability part consists only one method: TraceLinkValue(), that will process strings with user-specified technique. This method intends to support 6 different techniques: \n",
    "\n",
    "\n",
    "VSM\n",
    "\n",
    "orthogonal\n",
    "\n",
    "JS\n",
    "\n",
    "LDA\n",
    "\n",
    "LSA\n",
    "\n",
    "word2vec\n",
    "\n",
    "doc2vec\n",
    "\n",
    ">Currently only word2vec and doc2vec are inplementated. The implementations are in notebook 3.4_facade.ipynb and corresponind generated facade.py file. Actual implementation of word2vec and doc2vec are in the notebook 3.2_mining.unsupervised.eval.ipynb and corresponding nbdev generated eval.py. As the diagram shows, 3.4_facade.ipynb imports eval.py for instantiation of either word2vec or doc2vec class object. Those object then load \"*.model\" file and start calculation. \n",
    "\n",
    "To further add implementations for other techniques, programmers should modify notebook 3.4_facade.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "The analysis part of the API consists 9 methods: \n",
    "\n",
    "NumDoc()\n",
    "\n",
    "VocabShared()\n",
    "\n",
    "VocabSize()\n",
    "\n",
    "Vocab()\n",
    "\n",
    "SharedVocabSize()\n",
    "\n",
    "AverageToken()\n",
    "\n",
    "CrossEntropy()\n",
    "\n",
    "KLDivergence()\n",
    "\n",
    "MutualInformation()\n",
    "\n",
    "\n",
    "\n",
    "Currently only KLDivergence() and MutualInformation() are NOT inplementated. The implementations are in notebook 3.4_facade.ipynb and corresponind generated facade.py file. \n",
    "\n",
    "All methods in this section takes in pandas dataframe(s) as input. \n",
    "\n",
    ">NumDoc() method are simple enough to stand on its own, it just count the number of rows in dataframes. \n",
    "\n",
    ">Functions VocabShared(), VocabSize(), Vocab(), SharedVocabSize(), AverageToken() only need a sentencepiece processor bpe model to function. All these methods instantiate a processor by a \"*.model\" file and receive a Counter object, in which results are stored.\n",
    "\n",
    ">Actual implementation of CrossEntropy() is in notebook 1.0_exp.i.ipynb as dit_shannon(). CrossEntropy() simply combines two user-provided dataframes and process it through sentencepiece processor and calls this method with resulting Counter object. \n",
    "\n",
    "To further add implementations for KLDivergence() and MutualInformation(), notebook 3.4_facade.ipynb should be modified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API is deployed to pypi at https://pypi.org/project/ds4se/. \n",
    "\n",
    "To deploy future version of the API, follow steps listed below:\n",
    "\n",
    "    1) open setting.ini and increment the version number. \n",
    "    \n",
    "    2) open terminal and run following commmad to package the library:\n",
    "> python3 setup.py sdist bdist_wheel \n",
    "    \n",
    "    3) run following command to upload the package:\n",
    "  >twine upload dist/*\n",
    "  \n",
    "    4) when promoted for username, type in username:\n",
    "   >ds4se\n",
    "   \n",
    "    5) when promoted for password, type in username:\n",
    "   >ds4seCS435\n",
    "   \n",
    "   \n",
    "Note: you might need to run the following commands to make sure you have the latest version of setuptools, wheel and twine:\n",
    "  > python3 -m pip install --user --upgrade setuptools wheel\n",
    "  \n",
    ">python3 -m pip install --user --upgrade twine\n",
    "\n",
    "To include non-\".py\" file in the package, modified package_data variable in setup.py. For example, if you want to include \"hello.model\" and \"world.csv\" in the package, package_data should be:\n",
    ">package_data={'': ['hello.model','world.csv']},\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install ds4se`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se.facade as facade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the ds4se library to calculate trace link value of proposed trace link with given.The function will takes in two strings for contents for source file and target file, feed two strings into a model that user specifies, and return traceability value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Supported technique model:\n",
    "        VSM\n",
    "        LDA\n",
    "        orthogonal \n",
    "        LSA\n",
    "        JS\n",
    "        word2vec\n",
    "        doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a tuple of two integers, with the first element as distance between two artifacts and the second element be the similarity between two artifacts, which is the traceability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 22:55:01,937 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-01 22:55:01,947 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-01 22:55:01,949 : INFO : loading Word2Vec object from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model\n",
      "2020-11-01 22:55:01,997 : INFO : loading wv recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-01 22:55:01,998 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-01 22:55:01,999 : INFO : loading vocabulary recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-01 22:55:01,999 : INFO : loading trainables recursively from c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-01 22:55:02,001 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-01 22:55:02,002 : INFO : loaded c:\\users\\admin\\desktop\\fall2020\\software engineering\\project\\github desktop\\ds4se\\ds4se\\model\\word2vec_libest.model\n",
      "2020-11-01 22:55:02,015 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-01 22:55:02,019 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x000001F77D3A65B0>\n",
      "2020-11-01 22:55:02,020 : INFO : iterating over columns in dictionary order\n",
      "2020-11-01 22:55:02,022 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-01 22:55:02,167 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-01 22:55:02,227 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-01 22:55:02,235 : INFO : Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-11-01 22:55:02,236 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-01 22:55:02,238 : INFO : built Dictionary(4 unique tokens: ['content', 'file', 'one', 'string']) from 2 documents (total 7 corpus positions)\n",
      "2020-11-01 22:55:02,239 : INFO : Computed distances or similarities ('source', 'target')[[0.12804699828021432, 0.88648788705131]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.12804699828021432, 0.88648788705131)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facade.TraceLinkValue(\"source_string is a string of entire content of one source file\",\"target_string is a string of entire content of one targetfile\",\"word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec_metric is an optional parameter when using word2vec as technique, available metrics are: \n",
    "   <br> WMD\n",
    "  <br>  SCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data analysis part of ds4se library, users can use the library to conduct analysis on artifacts with information theory and statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all functions in analysis part, input should be pandas dataframe with following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            contents\n",
      "0                        hello world\n",
      "1  this is a content of another file\n"
     ]
    }
   ],
   "source": [
    "d = {'contents': [\"hello world\", \"this is a content of another file\"]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of ds4se model to calculate the number of documents of either source or target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The method can process dataframes for artifacts contents and return the number of documents each artifacts class contains. \n",
    "    It takes in two parameters, a pandas dataframe for source artifacts and a pandas data frame for target artifacts, and it will do calculation for both classes.\n",
    "    \n",
    "    The method returns a list of 4 integers:\n",
    "    1: number of documents for source artifacts;\n",
    "    2: number of documents for target artifacts;\n",
    "    3: source difference (difference between previous two results);\n",
    "    4: target difference (same as above, but opposite sign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents for source is 2 , with 0 source difference\n",
      "The number of documents for target is 2 , with 0 target difference\n"
     ]
    }
   ],
   "source": [
    "result = facade.NumDoc(source_df, target_df)\n",
    "source_doc = result[0]\n",
    "target_doc = result[1]\n",
    "difference_source = result[2]\n",
    "difference_target = result[3]\n",
    "print(\"The number of documents for source is {} , with {} source difference\".format(source_doc, difference_source))\n",
    "print(\"The number of documents for target is {} , with {} target difference\".format(target_doc, difference_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of ds4se model to calculate the vocabulary size of either source or target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The method can process dataframes for artifacts contents and return the total number of vocab contained in each artifact class. \n",
    "    The method takes in two parameters, source artifacts and target artifacts, and it will do calculation for both classes.\n",
    "    \n",
    "    The method returns a list of 4 integers:\n",
    "    1: vocabulary size for source artifacts;\n",
    "    2: vocabulary size for target artifacts;\n",
    "    3: source difference;\n",
    "    4: target difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for source is 10 , with 0 target difference\n",
      "The vocabulary size for target is 10 , with 0 target difference\n"
     ]
    }
   ],
   "source": [
    "vocab_result = facade.VocabSize(source_df, target_df)\n",
    "source = vocab_result[0]\n",
    "target = vocab_result[1]\n",
    "difference_source = vocab_result[2]\n",
    "difference_target = vocab_result[3]\n",
    "print(\"The vocabulary size for source is {} , with {} target difference\".format(source, difference_source))\n",
    "print(\"The vocabulary size for target is {} , with {} target difference\".format(target, difference_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of ds4se model to calculate the average number of token of either source or target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The method can process dataframes for artifacts contents and return the average number of tokens in each artifact class. \n",
    "    It does calculation by first finding the total number of token for each artifact class, and then divide each of them by the number of documents present in each artifacts.\n",
    "    The method takes in two parameters, source artifacts and target artifacts, and it will do calculation for both classes.\n",
    "    \n",
    "    The method returns a list of 4 integers:\n",
    "    1: average number of token for source artifacts;\n",
    "    2: average number of token for target artifacts;\n",
    "    3: source difference;\n",
    "    4: target difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of average token for source is 107 , with 35 source difference\n",
      "The number of average token for target is 143 , with -35 target difference\n"
     ]
    }
   ],
   "source": [
    "token_result = facade.AverageToken(source_df, target_df)\n",
    "source = token_result[0]\n",
    "target = token_result[1]\n",
    "difference_source = vocab_result[2]\n",
    "difference_target = vocab_result[3]\n",
    "print(\"The number of average token for source is {} , with {} source difference\".format(source, difference_source))\n",
    "print(\"The number of average token for target is {} , with {} target difference\".format(target, difference_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of ds4se model to retriev term frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The method can process dataframes for artifacts contents and return the top three most frequent terms that appears in artifact class. It employs bpe model to precess the contents in each dataframe\n",
    "\n",
    "    The method takes in two parameters, \n",
    "    1: source artifacts,\n",
    "    2: target artifacts, \n",
    "    and it will do calculation for both classes.\n",
    "    \n",
    "    The method returns a dictonary with \n",
    "    key: token\n",
    "    value: a list of count and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'est': [160, 0.16], 'http': [136, 0.136], 'frequnecy': [124, 0.124]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facade.VocabShared(source_df,target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If user only need the term frequency of one of two classes, they can choose to use Vocab() function, which is exactly the same except Vocab only processes one dataframe for one artifact class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'est': [141, 0.141], 'http': [136, 0.136], 'frequnecy': [156, 0.156]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facade.Vocab(artifacts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Shared Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following metrics to compute using both source and target artifacts, use the following funtions. \n",
    "\n",
    "For all methods below, two parameters are required: source and target artifacts, they are all in form of dataframes\n",
    "\n",
    "They all return one integer value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return the totla vocab size of source and target combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facade.SharedVocabSize(source_df, target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facade.MutualInformation(source_df, target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropy calculates shanno entropy of combind source and target artifacts, it returns a integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facade.CrossEntropy(source_df, target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facade.KLDivergence(source_df, target_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
